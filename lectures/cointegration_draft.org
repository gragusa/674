Cointegration

* Introduction
  Look at VAR(1): $y_t = a_0 + A y_{t-1} + e_t$; remember that if we
  want something like stationarity, we need to look at the roots of
  the polynomial $\det(I - A z) = 0$; nonstationary if there are roots
  on the unit circle (ie $|z| = 1$);

  possibilities:
  * $n$ roots ($n$ is the number of equations) on the unit circle
  * less than $n$, but positive
  * No roots on unit circle

  We haven't formally defined what it means for a process to be $I(0)$
  yet, but it captures what we usually mean when we say that a series
  is stationary.  If a series is stationary, we typically expect that
  it obeys a CLT.  But it is straightforward to write down stationary
  series that don't (for example, let $u_t$ be an i.i.d. sequence;
  then $\Delta u_t$ is stationary but obviously does not obey a CLT).

  So we say that the series $u_t$ is $I(0)$ if it has the MA
  representation $u_t = \Phi(L) \epsilon_t$ where $\epsilon_t$ is
  white noise, $\Phi(1) \neq 0$, and $\sum_{j=0}^\infty | \Phi_j |$ is
  finite (element-by-element in the case of multivariate processes).

  Now we extend the definition to $I(d)$ recursively: for $d > 0$,
  $u_t$ is $I(d)$ if $\Delta u_t$ is $I(d-1)$ and is $I(-d)$ if
  $\sum_{j=-\infty}^\infty u_t$ is $I(1-d)$.

* Cointegration setup
  Let $\Pi = A - I$ and rewrite the VAR as
  \[
  \Delta  y_t = a_0 + \Pi y_{t-1} + e_t
  \]

  $\Pi$ may not have full rank, but if it has rank $r$, we can always
  write $\Pi = \alpha\beta'$ where $\alpha$ and $\beta$ both are $k ×
  r$ with full rank.  Then
  \[
  \Delta y_t = a_0 + \alpha\beta'y_{t-1} + e_t
  \]
  (essentially, this is the "Granger representation theorem")

* Why must it be $\beta'y_t$ that's $I(0)$?
  The intuition's easiest to see when $r=1$.  In that case, $\alpha$
  becomes just a vector of constants, so it can't possibly remove the
  stochastic trend.

  why do this:
  - LHS is I(0), so RHS must be I(0) as well.
  - That must mean that $\beta'y_{t-1}$ is I(0)
  - $r$ is called the "cointegrating rank" of the system
  - If you knew $\beta$ this would be straightforward to estimate
    (sometimes you do: log oil prices and log jet fuel prices, for
    example)
  - $\alpha$ and $\beta$ are not unique (but might be with reasonable
    restrictions)

  More general error-correction form
  \[
  \Delta  y_t = a_0 + \alpha\beta' y_{t-1} + \pi(L) \Delta  y_{t-1} + e_t
  \]

* Interpreting cointegration
  - We'll sometimes know $\beta$ (at least under the null), in which
    case the analysis is "easy"
  - cointegrating vectors can represent long-run equilibria

* Working with known cointegrating vectors.

  Note that "known" can mean, as always, "known under the null
  hypothesis of interest.  So this material is at least potentially
  relevant even in real applications.

  If $\beta$ is known, estimating the VECM is completely straightforward.
  If it is of interest to test whether $y_t$ is cointegrated for a
  particular value of $\beta$, we can define $u_t = \beta'y_t$.
  - If $y_t$ is cointegrated (for that value of $\beta$), then
    $\beta'y_t$ is $I(0)$.
  - If it is not cointegrated (for that value of $\beta$), then
    $\beta'y_t$ has a unit root.
  We can then test for a unit root as usual.

* Working with unknown cointegrating vectors.
  This is the Engle-Granger approach; let's say for now that $r$ is 1
  and that $n$ (the number of series) is 3, and furthermore, suppose
  that we know the first variable has to be in the cointegrating
  relationship, so $\beta_1 \neq 0$.  If we define $v_t = \beta'y_t$,
  we can divide by $\beta_1$ to get the relationship
  \begin{equation}\label{eq:1}
     y_{1t} = \tfrac{1}{\beta_1} v_t - \tfrac{\beta_2}{\beta_1} y_{2t} - \tfrac{\beta_3}{\beta_1} y_{3t}.
  \end{equation}
  We can assume without loss of generality that $\beta_1=1$ (having
  already assumed that it's nonzero).  So a reasonable question is,
  can we estimate the cointegrating relationship by
  estimating~\eqref{eq:1} with OLS?  And if we do, is the estimation
  error in this step going to affect subsequent inference?

  We'll do a simple version of the math.  Just as in the stochastic
  integration section, we're going to have to scale at a rate
  different than $\sqrt{T}$ for a nondegenerate limit distribution.
  We have (dropping the first element of $\beta$ since it's assumed
  to be 1)
  \begin{equation*}
    T(\hat \beta - \beta)
    = \Big( \tfrac{1}{T²} \sum_{t=1}^T e₂'y_t y_t'e₂ \Big)^{-1}
      \tfrac{1}{T} \sum_{t=1}^T e₂'y_t \Delta y_t'e_1
  \end{equation*}
  where
  \begin{align*}
    e_1 &= \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} &
    e_2 &= \begin{pmatrix} 0 & 0 \\ 1 & 0 \\ 0 & 1 \end{pmatrix}
  \end{align*}
  are used to pick off the regressand and the two regressors as in
  the previous sections.

  Now, since
  \[
  \tfrac{1}{T} y_{[\lambda T]} y_{[\lambda T]}' \to^d \Sigma^{1/2} W(\lambda)W(\lambda)' \Sigma^{1/2\prime},
  \]
  and
  \begin{align*}
    \tfrac{1}{T} \sum_{t=1}^T y_t \Delta y_t
    &= \tfrac{1}{T} \sum_{t=1}^T y_{t-1} \Delta y_t' + \tfrac{1}{T} \sum_{t=1}^T \Delta y_t \Delta y_t' \\
    &\to^d \Sigma^{1/2} \int_0^1 W(\lambda) dW(\lambda) \Sigma^{1/2\prime} + \Sigma
  \end{align*}
  (assuming MDS) this converges to something that's $O_p(1)$, so it's
  super-consistent.

  An implication of super-consistency is that we can ignore
  estimation error in $\beta$ when we do inference.
  \begin{align*}
    \sqrt{T} (\hat \alpha \hat \beta' - \Pi)
    &= \sqrt{T} ((\hat \alpha - \alpha + \alpha) (\hat \beta - \beta + \beta)' - \alpha\beta') \\
    &= \sqrt{T} ((\hat \alpha - \alpha) \beta' + (\hat \alpha - \alpha) (\hat \beta - \beta) + \alpha (\hat \beta - \beta)') \\
    &= \sqrt{T} (\hat \alpha - \alpha) \beta' + o_p(1).
  \end{align*}
  Without superconsistency, $\sqrt{T} \alpha (\hat \beta - \beta)'$
  would not be $o_p(1)$.

  So the idea here would be to estimate $\beta$ first by OLS, then
  plug in the estimate as though $\beta$ were known and proceed as
  before.

  The obvious problem is that this all falls apart if it happens that
  $\beta_1=0$.  Since nothing about the above argument really
  requires that it be the /first/ element that's known to belong in
  the relationship, the issue is how to proceed if we aren't
  necessarily sure that any of the elements of $y_t$ is guaranteed to
  be part of the cointegrating relationship.

  In that case, you'd use Johansen's reduced rank regression; fit the
  model with OLS (or MLE) under the constraint that $\Pi =
  \alpha\beta'$ has reduced rank $r$.  Operationally, this requires
  - First, test the null that $\Pi = 0$ in
    \[
    \Delta y_t = a_0 \Pi y_{t-1} + \pi(L) \Delta y_{t-1} + e_t,
    \]

    i.e. test the null hypothesis that there is no cointegration
    (we've already decided/determined that $y_t$ is $I(1)$.
  - If you reject the first null, test the null that $r=1$ against
    the alternative that $r>1$.
  - If you reject, continue testing $r=j$ vs $r>j$ for
    $j=2,3,\dots,n-1$.
  - When you finally fail to reject, set that value for $r$.

  Obviously, these pre-testing procedures are really problematic in
  finite samples.

* First-differenced cointegrated processes do not have VAR representations.

  Suppose we wanted to just estimate the first differenced model
  (i.e. ignore the cointegration)

  $\Delta y_t$ is stationary, so can't we just invoke Wold representation
  theorem:
  \[
  \Delta  y_t = C(L) e_t
  \]
  Use Beveridge-Nelson decomposition ($C^*_j = - \sum_{s=j+1}^\infty C_s$)
  \[
  \Delta y_t = C(1) e_t + C^*(L) (e_t - e_{t-1})
  \]
  so
  \[
  y_t = y_0 + \sum_{t=0}^t \Delta  y_t = y_0 + C(1) w_t + C^*(L) e_t
  \]
  w/ $w_t = \sum_{s=0}^t e_t$ (a unit root process)

  Now, cointegration implies that $\beta'y_t$ is I(0), so
  \[
  \beta'y_0 + \beta'C(1) w_t + \beta'C^*(L) e_t
  \]

  must be I(0) as well, which only happens if the $w_t$ term is a.s. zero,
  so we need
  \[
  \beta'C(1) = 0
  \]
  as a consequence of cointegration (and vice versa). This is
  actually a big deal. remember that for an MA($\infty$) to be
  invertible, we need the solutions to $\det(C(z)) = 0$ to all be
  outside the unit circle, which we just ruled out.  So $\Delta
  y_t$ *does not have a VAR representation*

  Also, $t^{-1/2} y_t$ has limiting variance of $C(1) \Sigma C(1)'$,
  which means that $\avar(T^{-1/2} \sum_{t=1}^T \Delta y_t)$ has the
  same asymptotic variance, which doesn't have full rank.

* License and copying
  Copyright (c) 2013-2014 Gray Calhoun. Permission is granted to copy,
  distribute and/or modify this document under the terms of the GNU
  Free Documentation License, Version 1.3 or any later version
  published by the Free Software Foundation; with no Invariant
  Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of
  the license is included in the file LICENSE.tex and is also
  available online at [[http://www.gnu.org/copyleft/fdl.html]].
