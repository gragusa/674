Introduction to time series and ARMA modeling

This material is covered extensively in standard textbooks,
i.e. \citet{Ha00} or \citet{Gr12}, and is covered especially well by
specialty time series books (\citealp{BD91} and \citealp{Ha94}).

* Basic definitions

- A /stochastic process/ is a collection of random variables (or
  random vectors) indexed by a parameter $t$ indicating time.

- Want to discuss how this can be viewed as a sequence of random
  variables or as a random function of $t$ (sample path).

Often we need to deal with /triangular stochastic arrays/ in a
formal sense. A stochastic array is essentially a stochastic process
indexed by more than one parameter.

** Motivation for stochastic array:
Suppose $\{y_t; t=1,...,n\}$ is a stochastic process where each $y_t$
is i.i.d standard normal. Let $s^2_n$ be the sample variance of this
process. Is $z_t = (y_t - \bar y) / s_n$ a stochastic process?  (no)

So we introduce two sets of indices and write $z_{t,n} = (y_t - \bar
y) / s_n$; where $n = 1,2,...$ and $t = 1,...,n$.  Now $\{z_{t,n}\}$
is a stochastic array.

- The stochastic process $\{y_t\}$ is a /strictly stationary time series/
  if, for all finite $s$, $t$, and $k$, we have
  \begin{equation*}
    (y_s,...,y_{s+k}) =^d (y_t,...,y_{t+k}).
  \end{equation*}

- Suppose that $y_t$ is i.i.d.; then it is strictly stationary.

- Suppose that, for any $t$, $(y_t, y_{t+1}, y_{t+2})$ is distributed
  $N(\mu,\Sigma)$, and that $y_t$ and $y_{t+3}$ are independent for
  all $t$; then $y_t$ is obviously strictly stationary.

- Suppose that $\Delta y_t$ is i.i.d. $N(0,1)$ and $y_0$ is zero. Then
  $\E y_i = 0$ for all $i$, but
  \begin{equation*}
    \var(y_i) = \sum_{t=1}^i \var(\Delta y_t) = i
  \end{equation*}
  so the series is not stationary.

- Other examples too (e.g. breaks)

Strict stationary is usually "unnecessarily" restrictive; usually
asymptotic properties come from the first two moments, so we have a
variation of stationarity that applies to the first two moments.

- The stochastic process $\{y_t\}$ is a /weakly stationary time series/ if
  \begin{equation*}
    \E y_t = \mu
  \end{equation*}
  for all $t$ and
  \begin{equation*}
    \cov(y_t, y_{t+k}) = \cov(y_s, y_{s+k})
  \end{equation*}
  for all $s$, $t$, and $k$.
  Informally, the first two moments do not depend on $t$.

The first two moments of a weakly stationary process must
exist, which is not true of a strictly stationary process, so
it is not strictly weaker than strict stationarity.

- The /autocovariance/ function of a weakly stationary time
  series is defined as
  \begin{equation*}
    \Gamma(k) = \cov(y_t, y_{t-k}).
  \end{equation*}

- The /autocorrelation/ function of a weakly stationary time
  series is defined as
  \begin{equation*}
    \Rho(k) = \var(y_t)^{-1} \Gamma(k)
  \end{equation*}
"Weak dependence" requires that $\Gamma(k) \to 0$ as $k \to \infty$.

  Obvious properties if $\Gamma(\cdot)$ is the autocovariance function of
  $y_t$:
  - $\var(y_t) = \Gamma(0)$
  - $\Gamma(k) = \Gamma(-k)'$ for all $k$.
  
  Properties if $y_t$ is univariate
  - $|\gamma(k)| \leq \gamma(0)$ for all $k$. (This is an
    immediate consequence of the Cauchy-Schwarz inequality.)
  - The autocovariance function is /nonnegative definite/.

- A real-valued function f is nonnegative definite if, for any
  natural number $n$, any $a \in \RR^n$ and any $t \in \ZZ^n$, we have
    \begin{equation*}
      \sum_{i=1}^n \sum_{j=1}^n a_i a_j f(t_i - t_j) \geq 0.
    \end{equation*}

In this part of the class, we are going to assume that the series are
already stationary (weak, usually) and are weakly-dependent, for a few reasons:
- Statistical agencies typically provide seasonally-adjusted data
  and the data are usually aggregated enough that holidays, etc. are
  ignored.
- Accounting for trends and permanent shocks is a big deal in
  macro; we're going to talk about it at length later in the semester.

* Lag polynomials:

- The lag operator $\Lag$ is defined as the operator such that
  $\Lag y_t = y_{t-1}$

- The inverse is $L^{-1} y_t = y_{t+1}$.
- The lag operator behaves formally like premultiplication by a
  constant:
  - $\Lag(c y_t) = c y_{t-1} = c \cdot \Lag y_t$.
  - $\Lag (a + y_t) = a + y_{t-1} = a + \Lag y_t$.
  - $\Lag^k y_t = \Lag(\cdots \Lag(\Lag(\Lag y_t)) \cdots) =
    \Lag(\cdots \Lag(\Lag y_{t-1}) \cdots) = y_{t-k}$
- Lets us write "lag polynomials": if we define the polynomial
  $\theta(z) = \sum_{i=0}^k \theta_i z^i$, then
  \begin{equation*}
    \theta(\Lag) y_t = \sum_{i=0}^k \theta_i y_{t-i}.
  \end{equation*}
- Quick detour: how would we handle infinite sums? The question
  we'd want to answer is, if we define the new stochastic array
  $\{z_{kt}\}$ with
  \begin{equation*}
    z_{kt} = \sum_{i=0}^k \theta_i y_{t-i},
  \end{equation*}
  is $\lim_{k \to \infty} \{z_{kt}\}$ a stochastic process?

  Obviously not for all polynomials: if $\theta_i = 1$ and $y_{t-i}$
  is i.i.d., then $z_{kt} "\to" \sum_{i=0}^{\infty} y_{t-i}$ which
  is not tight.

  It's easier to check for weakly-dependent sequences: is
  $\theta(\Lag) y_t$ weakly-dependent?... (need to fill in details)

** Quick introduction to AR and MA processes

- As a starting point, we are going to build dynamic processes as
  linear functions of "white noise."
- Approach is analogous to what we did with OLS: we will talk
  about a statistical model that seems pretty narrow, but show how to
  get economic content out of it and why it might hold approximately
  in a general setting.

- A stochastic process $\{e_t\}$ is $WN(0, \sigma^2)$ if each
  $e_t ∼ (0, \sigma^2)$ and $\cov(e_t, e_s) = 0$ when $s \neq t$.

Obviously, the same definition applies when the $e_t$ are random
vectors.

To build $y_t$ from a white noise process $e_t$, there are two obvious
approaches:
- Let $y_t$ depend on $e_t$ as well as past $e_s$ ($s < t$).
- Let $y_t$ depend on past $y_s$ ($s < t$).
- We're going to (for now) build $y_t$ from linear functions of
  $e_t$ and past $y_t$.

- $y_t$ is an MA($q$) if it is a stationary solution to the equation
  \begin{equation*}
    y_t = \mu + \theta(L) e_t
  \end{equation*}
  where $e_t$ is white noise, $\mu$ is a constant, $\theta(z)$ is a
  polynomial of order $q$ s.t. $\theta(0) = 1$.

- $y_t$ is an AR($p$) if it is a stationary solution to the
  equation
  \begin{equation*}
    \phi(L) y_t = \mu + e_t
  \end{equation*}
  where $e_t$ is white noise, $\mu$ is a constant, and $\phi$ is a
  polynomial of order $p$ s.t. $\phi(0) = 1$.

- $y_t$ is a ARMA($p$,$q$) process if it is the
  stationary solution to the difference equation
  \begin{equation*}
    \phi(L) y_t = \mu + \theta(L) e_t
  \end{equation*}
  where $e_t$ is white noise, $\mu$ is a constant, and $\phi$ and
  $\theta$ are polynomials of order $p$ and $q$ such that $\phi(0) =
  \theta(0) = 1$.

*** Invertibility of lag polynomials

Suppose $\theta(z) = (1 -
\theta z)$ and $|\theta| < 1$. Then we can invert the polynomial:
\begin{equation*}
  (1 - \theta z)^{-1} = \sum_{j=0}^\infty \theta^j z^j
\end{equation*}
as long as $z \neq \theta^{-1}$. (PROOF OF EXISTENCE?)

Arguments similar to before imply that the limit is well defined, and
\begin{align*}
  (1 - \theta z) \sum_{j=0}^\infty \theta^j z^j
  &= \sum_{j=0}^\infty \theta^j z^j - \sum_{j=1}^\infty \theta^j z^j \\
  &= \theta^0 z^0 \\
  &= 1
\end{align*}
so this summation satisfies the definition of $(1 - \theta z)^{-1}$.
If we have a second order lag polynomial, so
$\theta(z) = (1 + \lambda_1 z) (1 + \lambda_2 z)$, then we can invert
both pieces separately as long as $|\lambda_1|$ and $|\lambda_2|$ are
both less than 1. For the general case, we can invert a lag polynomial
$\theta(z)$ as long as all of its roots lie outside the unit circle,%
\footnote{The roots can be complex in general, even when the polynomial
  coefficients are real.} %
so $\theta(z) \neq 0$ for all $z$ s.t. $|z| \leq 1$.

- Suppose that $y_t$ is the MA($q$) process $y_t = \theta(L) e_t$. If
  $\theta(L) \neq 0$ for all complex $z$ s.t. $|z| \leq 1$, then $y_t$
  is an /invertible/ MA process and can be expressed as the
  AR($\infty$) process: $\theta(L)^{-1} y_t = e_t$. This result
  continues to hold if $\theta$ is an infinite-order polynomial.

- Suppose that $y_t$ satisfies $\phi(L) y_t = e_t$. If $\phi(z) \neq
  0$ for all (complex) $z$ s.t. $|z| \leq 1$, then $y_t$ is a
  /causal/ AR process and can be expressed as the MA($\infty$):
  \[y_t = \phi(L)^{-1} e_t.\]
  This result continues to hold if $\phi$ is an infinite-order
  polynomial.

- If $\phi(z) = 0$ or $\theta(z) = 0$ for $|z| = 1$ then the
  process is said to have a unit root. For $\phi(z) = 0$ this is
  analagous to the cumulative sums we looked at.
- If $\phi(z) = 0$ for $|z| = 1$, the order of the solution is the
  number of unit roots and will affect the series behavior.
- Side note: this all holds for vector-valued processes as
  well. Instead of $|\phi(z)| \neq 0$ for all $z$ s.t. $|z| \leq 1$,
  we need $\det(z) \neq 0$ for all such $z$.
- If $\phi(z)$ and $\theta(z)$ have the same roots/zeros, so
  $\phi(L) = a(L) b(L)$ and $\theta(L) = a(L) c(L)$ then we can
  typically factor out and remove $a(L)$, giving a VARMA process with
  the same dynamics: $b(L) y_t = \mu^* + c(L) e_t$; so we typically
  assume that there are no common roots

*** Moments, covariance of ARMA processes

- For MA($q$), mean
  \begin{align*}
    \E y_t &= \mu + \E e_t + \sum_{i=1}^q \theta_i \E e_{t-i} \\
    &= \mu.
  \end{align*}
  and autocovariances
  \begin{align*}
    \cov( y_t, y_{t-j}) &= \E \sum_{i=0}^q \theta_i e_{t-i} \sum_{k=0}^q e_{t-j-k} \theta_k\\
    &= \sum_{i=j}^q \theta_{i+j} \theta_i \sigma^2 \\
    &= \sigma^2 \sum_{i=0}^q \theta_i \theta_{i+j}.
  \end{align*}
  This is zero if $j \geq q$ (note that if $e_t$ is Normal, $MA(q)$
  processes are $q$-dependent). This remains finite as $q \to \infty$
  as long as $\sum_{j=0}^\infty \theta_i^2$ is finite.
- For AR($p$), mean: suppose that the process is covariance
  stationary. Then $\E y_t = \E y_{t-i}$ and so
  \begin{equation*}
    \E y_t = \mu + \sum_{i=1}^p \phi_i \E y_t + 0.
  \end{equation*}
  If $1 - \sum_{i=1}^p \phi_i$ is invertible, then we have
  \begin{equation*}
    \E y_t = (I - \sum_{i=1}^p \phi_i )^{-1} \mu
  \end{equation*}
  (invertibility of this matrix turns out to be a necessary condition
  for covariance stationarity).

  For the autocovariances assume 0-mean, covariance stationary. Since
  \begin{equation*}
    y_t = \sum_{i=1}^p \phi_i y_{t-i} + e_t,
  \end{equation*}
  we can post-multiply by $y_{t-j}$ to get
  \begin{equation*}
    y_t y_{t-j} = \sum_{i=1}^p \phi_i y_{t-1} y_{t-j} + e_t y_{t-j}
  \end{equation*}
  and (assuming $e_t$ and $y_{t-j}$ are uncorrelated)
  \begin{equation*}
    \E y_t y_{t-j} = \sum_{i=1}^p \phi_i \E y_{t-i} y_{t-j}.
  \end{equation*}

  This gives a recursive definition for the autocovariances. To get
  the initial conditions, we have $(j = 1,\dots,p)$
  \begin{align*}
    \gamma(0) &= \sum_{i=1}^p \phi_i \gamma(i) + \sigma^2 \gamma(j) \\
    \gamma(j) &= \sum_{i=1}^p \phi_i \gamma(j-i).
  \end{align*}
  This gives us a system of equations that can
  be solved for $\gamma(0),\dots,\gamma(p)$. Then, for higher-order autocovariances,
   we have the relationship (if $j > p$)
   \begin{equation*}
     \gamma(j) = \sum_{i=1}^p \phi(i) \gamma(j-i),
   \end{equation*}
   which can be solved recursively or explicitly through difference
   equations.

   Note that the autocovariance dies out slowly over time for AR
   processes.
- For ARMA, see book...

*** Canonical representation of a VAR(p):
If we define:
\begin{align}
  w_t &= (y_t, y_{t-1}, \dots, y_{t-p+1})' \\
  u_t &= (e_t, 0, \dots, 0)' \\
  \Psi &= \begin{pmatrix}
    \phi_1 & \phi_2 & \phi_3 & \cdots & \phi_{p-1} \phi_p \\
    I & 0 & 0 & \cdots & 0 & 0 \\
    0 & I & 0 & \cdots & 0 & 0 \\
    \vdots \\
    0 & 0 & 0 & \cdots & I & 0
  \end{pmatrix}
\end{align}
Then we can write a VAR($p$) model as the VAR(1):
\[w_t = \Psi w_{t-1} + e_t\]

** Dynamics:

For *univariate* time series, people typically focus on
- Autocorrelation Function (ACF)
- Partial Autocorrelation Function (PACF)

This holds whether or not the true DGP is thought to be an AR or MA
process.

- The Autocorrelation function is just $\rho(j)$ viewed as a function of
  $j$; we've already calculated the autocovariance for $AR(p)$ and
  $MA(q)$ processes, so the autocorrelation is just a matter of
  scaling.
- The $j$th partial autocorrelation of a weakly stationary series is
  defined as $\corr(y_t, y_{t-j} ∣ y_{t-1},\dots, y_{t-j+1})$.

  Define this as $\alpha(j)$ for now

  The partial autocorrelation function is $\alpha(j)$ as a function of $j$.

  Analogously to OLS, we know that $\alpha(j)$ is the coefficient $\beta_j$ on
  $y_{t-j}$ in the equation
  \[y_t = \beta_0 + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \cdots + \beta_j y_{t-j} + u_t\]
  where the $\beta_i$ minimize the population MSE.

  Solution method: $(\alpha(0), \alpha(1)) = (\gamma(0), \gamma(1))$. Then it works just
  like for OLS:
  \[ y_t = ( y_{t-1}, y_{t-2},\dots, y_{t-i} ) \beta(i) + u_{t,i} \] giving
  \[( y_{t-1},\dots, y_{t-i} )'y_t = ( y_{t-1},\dots, y_{t-i} )' ( y_{t-1},\dots,
  y_{t-i} ) \beta(i) + ( y_{t-1},\dots, y_{t-i} )' u_{t,i}\]
  Take expectations and solve to get
  \begin{equation}
    \begin{pmatrix}
      \gamma(0) & \gamma(1) & \cdots & \gamma(i) \\
      \gamma(1) & \gamma(0) & \cdots & \gamma(i-1) \\
      \vdots \\
      \gamma(i) & \gamma(i-1) & \cdots & \gamma(0)
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      \gamma(1) \\ \gamma(2) \\ \vdots \\ \gamma(i+1)
    \end{pmatrix}
    =
    \begin{pmatrix}
      \beta_{i1} \\ \beta_{i2} \\ \vdots \\ \beta_{ii}
    \end{pmatrix}
  \end{equation}
  and take the last element of $\beta(i)$ as $\alpha(i)$.

  For an $AR(p)$, $\alpha(i) = 0$ for all $ \geq 0$

  For an $MA(q)$, the PACF dies out slowly as $i \to \infty$.

- You can trivially extend ACF and PACF to vector processes and
  the same results broadly hold.
- We're not going to worry about that now, because for
  multivariate dynamics we care mostly about finding structure that
  has an economic interpretation.
- Sample ACF and PACF can be used for modeling.

* Wold Decomposition: the generality of VAR, VMA, and VARMA processes

Introduce some probabilistic notation: define $F_t$ to be the sigma-field
(information set) generated by $y_t$, $y_{t-1}$, $y_{t-2},\dots$

- A sequence $\{F_t\}$ of sigma-fields is a /filtration/ if
  $F_{t-1} \subset F_t$ for all $t$.

  The tail sigma-field of a collection of sigma-fields $\{F_t;
  t=\dots,-2,-1,0,1,2,\dots\}$ is defined as $\bigcap_{n=-\infty}^\infty \bigcup_{t=n}^\infty F_t$

  For a filtration, we have $\bigcup_{t=n}^{n+k} F_t = F_{n+k}$ and
  $\bigcup_{t=n}^k \bigcap_{t=n+1}^k F_t = \bigcup_{t=n+1}^k F_t$, so the tail
  sigma-field is equivalent to
  \begin{align}
    \bigcap_{n=-\infty}^\infty \lim_{k \to \infty} \bigcup_{t=n}^{n+k} F_t
    &= \bigcap_{n=-\infty}^\infty \lim_{k \to \infty} F_{n+k} \\
    &= \lim_{l,k \to \infty} \bigcap_{n=-l}^\infty F_{n+k} \\
    &= \lim_{l,k \to \infty} F_{-l} \\
    &= \lim_{l \to -\infty} F_l \\
    &= F_{-\infty}
  \end{align}

- A process is /deterministic/ if it is perfectly predictable by linear
  combinations; i.e. $v_t$ is $G_{-\infty}$-measurable, with $G_{-\infty} = \lim
  G_n$ and
  \[G_n = \{c + \sum_{t \leq n} a_t X_t ∣ c, a_t \in R\}.\]

- Any zero-mean nondeterministic stationary process $\{y_t\}$ can be
  expressed as the sum of an $MA(\infty)$ process $u_t$ and a deterministic
  process $v_t$, so
  \[y_t = \sum_{j=0}^\infty \theta_j z_{t-j} + v_t\]
  with $z_{t-j}$ in $G_n$ and $v_t$ in $G_{-\infty}$; $z_t$ is the residual
  from projecting $y_t$ onto $G_{t-1}$, and $v_t = y_t - \theta(L) z_t$
  (this is the /Wald decomposition/)

  Now, if $\theta(L)$ is invertible then we can write this as a causal VAR,
  and if $\theta(L)$ can be factored, we can write the process as a causal
  VARMA.

* Parameter estimation

There are a few different approaches that we can take for estimation
- Method of moments
- Maximum Likelihood (and conditional MLE)
- Bayesian estimation

* Estimation for VAR parameters (no MA component).

Have the model (again):
\[y_t = \mu + \sum_{i=1}^p \phi_i y_{t-i} + e_t\] $k$ equations, $k p + 1$
regression coefficients. Under assumptions of causality, we know that
$e_t$ is uncorrelated with $y_s$ for $s < t$, which implies that we
might be able to justify OLS:
- Let $z_t' = (1 y_{t-1}' \cdots y_{t-p}')$
- $\Phi = [ \mu \phi_1 \dots \phi_p ]'$
- Then $y_t' = z_t'\Phi' + e_t'$ and we can do equation-by-equation
  OLS, which is equivalent to
  \[\hat \Phi' = (\sum_{t=p+1}^T z_t z_t')^{-1} \sum_{t=p+1}^T z_t y_t'\]

This is the method of moments estimator as well as the conditional MLE
estimator if you assume that $e_t ∼ N(0, \Sigma)$ and
condition on $y_1$,\dots,$y_p$. To do MLE, observe that
\[y_t ∣ z_t ∼ N(\Phi z_t, \Sigma).\]

The joint likelihood can be taken as the product of the conditional
likelihoods:
\[L(\Phi, \Sigma; y_1,\dots,y_T) = f_T(y_T ∣ y_{T-1},\dots, y_1) f_{T-1}(y_{T-1} ∣
y_{T-2},\dots, y_1) \cdots f_{p+1}(y_{p+1} ∣ y_p,\dots,y_1) f_p(y_p,\dots,y_1)\]

Assuming normality and correct specification,
\[f_t(y_t ∣ y_{t-1},\dots, y_1) = f_t(y_t ∣ z_t).\]

If you want, you can avoid conditioning on $y_p$,\dots,$y_1$ since the
joint dist of those observations is also normal and we've worked out
how to find the mean and variance.

* Estimating MA models

- This is a subject covered extensively by \citet{Ha94}, so we won't go
  into much detail here.
- There are two basic approaches:
  1. Assume $e_t$ is normal, then $y_1,\dots,y_T$ is jointly normal
    and you can maximize the likelihood w/rt $\Phi$, $\Theta$, and
    $\Sigma$

    This amounts to invoking invertibility to derive $f_T(y_t ∣
    y_{t-1},\dots,y_1)$ efficiently for each $t$ w/out using strict
    VAR structure, then building up the unconditional likelihood from
    that.

  2. Just like for VAR and VMA processes, we can derive equations
    that define the autocovariances at lags $0,\dots,p+q$ in terms of
    $\Phi$ and $\Theta$; solving for $\Phi$ and $\Theta$ and plugging
    in the sample autocovariances give the "Yule-Walker" estimates.

    This would be OLS for a VAR; when there's an MA component it can
    be inefficient.

* Asymptotic theory for time-series

Look at OLS estimator for VAR(p). As always, two components:
\[\hat \Phi' = \Phi' + (1/T \sum_{t=p+1}^T z_t z_t')^{-1} 1/T
\sum_{t=p+1}^T z_t e_t'\] so, for consistency need the sum of $z$'s to be
$O_p(1)$; second sum to be $o_p(1)$. For asymptotic distribution
(normality) need second summation to obey a CLT.

- Obviously, if we want to estimate the first or second moments of
  a time-series process, we might want to use the sample moments.
- Of interest on their own
- Necessary for consistency and asymptotic normality of MoM
  estimators
- Just look at univariate for simplicity.

- A sequence of sigma-fields $\{G_t\}$ is a /filtration/ if $G_t
  \subset G_{t+1}$ for all $t$

- The sequence of rvs $\{e_t\}$ is an mds w/rt the filtration $\{G_t\}$ if
  $E(e_t ∣ G_{t-1}) = 0$.

  Typically $G_t = \sigma(e_t, e_{t-1},\dots)$

  Comes from the definition of a martingale: $y_t$ is a martingale
  w/rt the filtration if $E(y_t ∣ G_{t-1}) = y_{t-1}$. Then $e_t =
  \Delta y_t$ is an mds.

- Now, suppose that $e_t$ has finite variance and is an mds w/rt
  $G_t = \sigma(y_t, y_{t-1},\dots)$, then we know that the second sum has mean
  zero. Also
  \begin{align*}
    E (1/T \sum z_t e_t)(1/T \sum e_s z_s')
    &= 2/T^2 \sum_{s \leq t} E(e_s e_t z_t z_s' ∣ G_{t-1}) \\
    &= 1/T^2 \sum_t E E( e_t^2 ∣ G_{t-1}) z_t z_t' + 2/T^2 \sum_{s < t} E E(
    e_t ∣ G_{t-1}) e_s z_t z_s' \\
    &= \sigma^2 1/T^2 \sum_t E z_t z_t'
  \end{align*}

  This converges to zero since $y_t$ has finite variance and $\Phih$
  converges in MSE to $\Phi$.
- Note that if $e_t$ is MDS, so is $z_t e_t$

* Standard MDS CLT (McLeish 1974; Hall and Heyde 1980)

Suppose that $Z_t$ is a (univariate) MDS and define
- $U_n^2 = \sum_1^n Z_t^2$,
- $s_n^2 = E U_n^2$.
If
- $U_n^2 / s_n^2 \to 1$ i.p. and
- $max_{1 \leq t \leq n} |Z_n / s_n| \to 0$ i.p.
then
\[1/s_n \sum_1^n Z_t \to^d N(0, 1)\]
or, equivalently,
\[1/U_n \sum Z_t \to^d N(0, 1)\]

For multivariate, let $\Omega_n = \sum E Z_t Z_t'$ and we have $\Omega_n^{-1/2} \sum
Z_t \to N(0, I)$ under essentially the same conditions.

Apply this to second summation in OLS coefficients and (assuming the
conditions hold) we can see that
\[n^{-1/2} \sum z_t e_t = ( \Omega_n / n )^{1/2} \Omega_n^{-1/2} \sum z_t e_t
\to^d N(0,\Omega) \]

where $\Omega_n = \sum E( e_t^2 z_t z_t' )$ and $\Omega = \lim \Omega_n / n$. Under
homoskedasticity, this can be simplified.

If $1/n \sum z_t z_t' \to V$ i.p., then $\lim \Omega_n = \sigma^2 V$ under
homoskedasticity and we have
\[\sqrt{n} ( \hat \phi - \phi ) \to^d N( 0, \sigma^2 V^{-1} ).\]

Under heteroskedasticity,
\[\sqrt{n} (\hat \phi - \phi ) \to^d N( 0, V^{-1} \Omega V^{-1} )\]

*For the asymptotic distribution of the VAR(p)* obviously it's
joint normal, so the only tricky thing is accounting for all of the
correlations. Rewrite it as
\[
sqrt(n) (\hat \Phi' - \Phi')
= (1/n \sum_{t=p+1}^n z_t z_t')^{-1} 1/sqrt(n) \sum_{t=p+1}^n z_t e_t'
\]
so
\[\sqrt{n} ( vec(\hat \Phi' ) - vec( \Phi' ) ) = vec( \dots )\]

and use the fact that, for conformable matrices $A$, $B$, and $C$,
\[vec( A B C ) = ( C \otimes A ) vec( B )\]
where $\otimes$ is the kronecker product and
\[
C \otimes A =
\begin{pmatrix}
  c_{11}A & \cdots & c_{1n}A \\
  \vdots
  c_{n1}A & \cdots & c_{nn}A
\end{pmatrix}
\]
$A = ( 1/n \sum z_t z_t' )^{-1}$, $B = 1/sqrt{n} \sum z_t e_t'$, $C = I$ so
$C \otimes A = diag( A,\dots, A )$

if $V = plim 1/n \sum z_t z_t' \Omega = plim 1/n \sum vec( z_t e_t' ) vec( z_t
e_t' )'$, then $\sqrt{n} vec(\hat \Phi' - \Phi' ) \to N( 0, diag(
V^{-1},\dots, V^{-1} ) \Omega diag(V^{-1},\dots,V^{-1} )$

Note that the block diagonal structure means that if we're interested
in the joint dist. of the coefficients for just a single equation,
it's not affected by those from other equations (only holds when all
of the equations have the same regressors\dots).

* Result (in summary)

Suppose that $y_t$ is a VAR(p),
- $1/n \sum z_t z_t' \to V$ i.p.
- $1/n \sum z_t z_t' e_t^2 \to \Omega$ i.p.
- $e_t$ is an mds with finite $4 + \delta$ moments ($\delta > 0$)
Then $\sqrt{T} ( \hat \Phi' - \Phi' )$ is asymptotically normal (with the
variance we derived).

* Laws of large numbers
We still need to talk about the square terms:
\[1/n \sum z_t z_t'\]
and
\[1/n \sum vec( z_t e_t' ) vec( z_t e_t' )'\]
- If $z_t$ is uniformly integrable (i.e. a slightly weaker condition than
  assuming it has $1 + \delta$ moments, $\delta > 0$) and MDS, then
  \[1/n \sum z_t \to^{L_1} 0\]

MDS for second moments is not usually an implication of Economic
Theory; is not usually a feature of a correctly specified model of the
mean; and is often false in Economic data. So it's worth mentioning
another result:

- If $\{z_t\}$ is a stationary and ergodic sequence with finite mean
  $\mu$, then
  \[1/n \sum z_t \to \mu\] (in $L_1$).

  Ergodic: a stationary sequence $\{x_t\}$ is /ergodic/ if, for
  any bounded functions $f$ and $g$, as $n \to \infty$,
  \[E f( x_t, \dots, x_{t + k}) g(x_{t + n}, \dots, x_{ t + n + l} ) \to E f(
  x_t, \dots, x_{ t + k } ) E g(x_{ t + n }, \dots, x_{ t + n + l} )\]

  - remember, you can prove independence by proving this
    factorization for all bounded $f$ and $g$, so this is an "asymptotic
    independence" condition;
  - Sometimes see conditions on how fast the convergence to zero
    happens
  - Nothing special about stationarity; you'll see other weak
    dependence conditions based on similar ideas (i.e. mixing
    conditions).

* CLT with serial correlation

Have a CLT for MDS. But what if we have an MA process?
\[y_t = \sum_{j=0}^\infty \theta_j e_{t-j} = \theta(L) e_t\]

- Just focus on the average: $\bar y$
- Obviously, if $\theta_j$ decays fast enough, LLN holds.
- Look at CLT:
  \begin{align*}
    T^{1/2} \bar y
    &= 1/T^{1/2} \sum_{t=1}^T \sum_{j=0}^\infty \theta_j e_{t-j} \\
    &= \sqrt{1/T} ( \theta_0 e_T +
    ( \theta_0 + \theta_1 ) e_{T-1} +
    ( \theta_0 + \theta_1 + \theta_2 )  e_{T-2} + ...
    ( \theta_0 + ... + \theta_{T-1} ) e_1 + ... ) \\
    &= \sqrt{1/T} ( ( \theta(1) - \sum_1^\infty \theta_j ) e_T +
    ( \theta(1) - \sum_2^\infty \theta_j ) e_{T-1} + ...
    ( \theta(1) - \sum_T^\infty \theta_j ) e_1 + ... ) \\
    &= \sqrt{1/T} \theta(1) \sum_{t=1}^T e_t + \sqrt{1/T} \theta^*(L) e_t
  \end{align*}
  where $\theta_j^* = - \sum_{i=j+1}^\infty \theta_i$.  Now $\theta(1)$ has the bulk of the
  dependence, so the first term obeys a CLT and the second can
  converge to zero i.p.

Result: Suppose that $y_t = \mu + \theta(L) e_t$ where
- $\sum_{s=0}^\infty s |\theta_s| < \infty$
- $1/\sqrt{T} \sum_t e_t \to N(0, \sigma^2)$ in distribution (i.e. any CLT
  holds)
Then $\sqrt{T} (\bar y - \mu ) \to N( 0, \sigma^2 \theta(1)^2 )$ in distribution

* Last bits

Forecasting from VAR:
- 1-step ahead $\hat y_{T+1} = \hat \phi(L) y_T$
- $k$-step ahead $\hat y_{T+k} = \hat \phi(L) y_{T+k-1} = \dots = ( \hat
  \phi(L) )^k y_T$
Also see forecasting by just regressing $y_{t+k}$ on $y_t,\dots,y_{t-p}$

What to do with these approximations?
- Test as normal
- Test for Granger causality/exogeneity
  - split up our variables into
    \begin{align*}
      y_{1t} &= \mu_1 + A_1 x_{1t} + A_2 x_{2t} + e_t \\
      y_{2t} &= \mu_2 + B_1 x_{1t} + B_2 x_{2t} + e_t
    \end{align*}
    where the $x_{it}$ contains lags of $y_{it}$.
  - $y_2$ does not Granger-cause $y_1$ if lags of $y_2$ do not
    help predict $y_1$ after accounting for lags of $y_1$ (also means
    that $y_1$ is exogenous w/rt $y_2$).
  - Means that $A_2 = 0$.
  - This is something we can test.

** Lag length selection

- $AIC_p = \log | \Sigma_u(p)| + 2 p m^2 / T$
- $BIC_p = \log | \Sigma_u(p)| + \log(T) 2 p m^2 / T$
- $\Sigma_u(q)$ is the vcv of $y_t - b_1 y_{t-1} - \dots - y_{t-p}$
- Choose to minimize the criterion
- Last term works as a penalty

* License and copying

Copyright (c) 2013-2014 Gray Calhoun. Permission is granted to copy,
distribute and/or modify this document under the terms of the GNU Free
Documentation License, Version 1.3 or any later version published by
the Free Software Foundation; with no Invariant Sections, no
Front-Cover Texts, and no Back-Cover Texts. A copy of the license is
included in the file LICENSE.tex and is also available online at
[[http://www.gnu.org/copyleft/fdl.html]].
