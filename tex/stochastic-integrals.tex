% Copyright © 2013, Gray Calhoun.  Permission is granted to copy,
% distribute and/or modify this document under the terms of the GNU Free
% Documentation License, Version 1.3 or any later version published by
% the Free Software Foundation; with no Invariant Sections, no
% Front-Cover Texts, and no Back-Cover Texts.  A copy of the license is
% included in the section entitled "GNU Free Documentation License."

\chapter{Stochastic integration}

\begin{itemize}
\item
  $W(λ)$ is a \emph{Weiner process} or \emph{Brownian Motion} if it is
  \begin{itemize}
  \item continuous and mean zero
  \item $W(t) - W(s) \sim N(0, t-s)$ for any $t$ and $s$
  \item Non-overlapping intervals are independent
  \end{itemize}
\item Draw diagram of sample paths w/ $\sqrt T$ envelope
\item Obviously, if $e_t$ is an MDS with variance 1, we have
  \[1/\sqrt{T} \sum_{t=1}^{[λT]} e_t →^d W(λ)\]
  under reasonable assumptions; want to be able to write this as an
  integral $\int_0^λ dW$

  \begin{itemize}
  \item
    $\lim 1/T \sum_{t=1}^{[λT]}$ maps to $\int_0^λ dt$
  \item
    $\lim e_t \sqrt{T}$ must map to $dW/dt$

    \begin{itemize}
    \item $e_t/\sqrt{T} ≈ dW$
    \item i.e. $dW^2 ≈ dt$
    \item You can also see this from writing out
      \[dW/dt = \lim_{h → 0} (W(t+h) - W(t)) / h\]
    \end{itemize}
  \item This is just intuition; proving is a little more difficult.
  \item Implies that $W(λ)$ is not differentiable a.e.
  \end{itemize}
\item We only need a few basic results; there are entire classes you
  can take on working with Ito integrals and SDEs
\end{itemize}

\section{Application \& use}

\begin{itemize}
\item Continuous mapping theorem: if $f$ is a continuous functional on
  $[0,1]$ then \[f(1/\sqrt{T} \sum_{t=1}^{[λT]} e_t) →^d f(W(λ))\]
  and, generally, if $g_t →^d g$ where $g$ is a random process on
  $[0,1]$ then $f(g_t) →^d f(g)$
\item Functional delta-method is similar
\item Also can use integration results
\end{itemize}

\section{Unit roots in regression}

\subsection{Spurious regression}

Suppose that we have a multivariate unit root process
\[y_t = y_{t-1} + e_t\] and we run the regression
\[y_{1t} = β y_{2,t-1} + u_t\]

Define

\begin{itemize}
\item $v_1 = (1, 0)'$
\item $v_2 = (0, 1)'$
\end{itemize}

The OLS coefficients can be written as
\begin{equation}
  \begin{split}
    \hat β &= \Big(\sum_{t=2}^T v_2' y_{t-1} y_{t-1}' v_2\Big)^{-1}
    \sum_{t=2}^T v_2' y_{t-1} y_t' v_1 \\
    &= \Big(v_2' \sum_{t=2}^T y_{t-1} y_{t-1}' v_2 \Big)^{-1} \Big(
    1/T^2 v_2' \sum_{t=2}^T y_{t-1} y_{t-1}' v_1
    + 1/T^2 v_2' \sum_{t=2}^T y_{t-1} e_t v_1 \Big) \\
    &→^d \Big(v_2' Σ^{1/2} \int_0^1 W(s) W(s)' ds Σ^{1/2}
    v_2\Big)^{-1} v_2' Σ^{1/2} \int_0^1 W(s) W(s)' ds Σ^{1/2} v_1
\end{split}
\end{equation}
This is interesting because it means that $\hat β$ is not consistent
(i.e.~doesn't converge to $β$). You can also show that $t$-stats don't
work (the statistic diverges so the test rejects with probability 1 in
the limit)

Relevant texts:
\begin{itemize}
\item Granger and Newbold (1974)
\item Phillips (1986)
\end{itemize}

\section{Regress I(1) on I(0)}

Take the last example, but suppose that we include a covariance
stationary I(0) regressor $x_{t-1}$ (but this is going to work for any
collection of I(0) regressors as well),
\[y_t = β_0 x_{t-1} + β_1 + β_2 y_{t-1} + u_t\] and say we want to get
limiting distributions for OLS. Without loss of generality, assume that
$E x_t = 0$; otherwise rewrite the equation as
\[y_t = β_0 (x_{t-1} - E x_{t-1}) + (β_1 + β_0 E x_{t-1}) + β_2 y_{t-1} + u_t\]
Also assume that $var e_t$ is 1 to keep the notation as simple as
possible.

How to get asymp dist of $\hat β - β$? Key thing is that the different
elements are going to converge at different rates; let
\[Λ = diag(\sqrt{T}, \sqrt{T}, T)\] so
\begin{equation}
\begin{split}
  Λ (\hat β - β) &=  \left( Λ^{-1} \sum_{t=2}^T
    \begin{pmatrix}
      x_{t-1}^2       & x_{t-1}   & x_{t-1} y_{t-1} \\
      x_{t-1}         & 1         & y_{t-1} \\ 
      x_{t-1} y_{t-1} & y_{t-1}   & y_{t-1}^2
    \end{pmatrix} Λ^{-1} \right)^{-1}
  Λ^{-1} \sum_{t=2}^T 
  \begin{pmatrix}
    x_{t-1} e_t \\ e_t \\ y_{t-1} e_t 
  \end{pmatrix}\\
  & \to^d
  \begin{pmatrix}
    E x_t^2 & 0             & 0 \\
    0       & 1             & \int_0^1 W(s) ds \\
    0       & \int_0^1 W(s) ds & \int_0^1 W(s)^2 ds
  \end{pmatrix}^{-1}
  \begin{pmatrix} (E x_t^2)^{1/2} W(1) \\ W(1) \\ \int_0^1 W(s) dW(s) \end{pmatrix}
\end{split}
\end{equation}
Where the 0 terms in the X'X matrix come from
\[T^{-1} \sum_{t=2}^T x_{t-1} →^p 0\] from the LLN and
\[T^{-1} \sum_{t=2}^T x_{t-1} y_{t-1} →^d \int_0^1 W_y(s) dW_x(s)\] so
\[T^{-3/2} \sum_{t=2}^T x_{t-1} y_{t-1} →^p 0.\]

Since the $X'X$ component is block diagonal, the asymptotic distribution
of the first element of $\hat β$ is $(E x_t^2)^{-1} W(1)$, making it
consistent and asymptotically normal at the usual rate. The coefficient
on the I(1) term and on the constant are both not Normal.

Note that I got a little too cute in class and forgot that the constant
term is \emph{not} I(0); note that $T^{-1} \sum_t 1$ obeys a ``LLN'' and
converges to 1, but $T^{-½} \sum_t 1$ definitely does not obey a ``CLT''.
So the estimator on $x_{t-1}$ is asymptotically normal, but the
estimator of the intercept is not.

\textbf{Additional lag structure}

Suppose now you run the regression
\[y_t = β_0 + β_1 y_{t-1} + β_2 y_{t-2} + u_t\]

\begin{itemize}
\item know that OLS estimator of $β_0$ is normal and correctly centered
\item we can rewrite the relationship as
  \[y_t = β_0 + β_1 Δy_{t-1} + (β_2 + β_1) y_{t-2} + u_t\] and
  estimating $β_1$ in this equation will give

  \begin{itemize}
  \item A numerically identical estimate as in the previous equation
  \item A consistent and asymptotically normal estimator of $β_1$
  \item Note that our estimate of $β_1 + β_2$ will have an awkward
    distribution
  \item So the OLS estimate of $β_1$ in the original regression is
    consistent and asymptotically normal
  \end{itemize}
\item Similarly, we can show that the OLS estimate of $β_2$ in the
  original regression is consistent and asymptotically normal.
\item Note that the estimate of $β$ is not jointly normal, since
  $β_1+β_2$ has a non-normal distribution.
\item This is true whenever you can rewrite the expressions so that
  coefficients appear on I(0) components and has implications for
  cointegration.
\end{itemize}

Relevant texts:
\begin{itemize}
\item Sims, Stock, and Watson (1990)
\end{itemize}

\section{Regress I(0) on I(1)}
To be added\ldots

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../textbook"
%%% End: 