% Copyright © 2013, Gray Calhoun.  Permission is granted to copy,
% distribute and/or modify this document under the terms of the GNU Free
% Documentation License, Version 1.3 or any later version published by
% the Free Software Foundation; with no Invariant Sections, no
% Front-Cover Texts, and no Back-Cover Texts.  A copy of the license is
% included in the section entitled "GNU Free Documentation License."

\chapter{Introductory lectures on time series and ARMA modeling}

Textbook material:
\begin{itemize}
\item Hayashi:
  \begin{itemize}
  \item Chapter 2 (2.2 especially)
  \item Chapter 6
  \end{itemize}
\item Greene 6th edition:
  \begin{itemize}
  \item Chapter 19
  \end{itemize}
\item Greene 7th edition:
  \begin{itemize}
  \item Chapter 20
  \end{itemize}
\end{itemize}

Other books that students may want to consult:
\begin{itemize}
\item Peter Brockwell and Richard Davis (1991), \emph{Time series:
    theory and methods, second edition}.
\item Jim Hamilton (1994), \emph{Time series analysis}.
\end{itemize}

\section{Basic definitions}

\begin{description}
\item[Stochastic process:]
A stochastic process is a collection of random variables (or
randomvectors) indexed by a parameter $t$ indicating time.
\end{description}

Want to discuss how this can be viewed as a sequence of random
variables or as a random function of $t$ (sample path).

Often we need to deal with \textbf{triangular stochastic arrays} in a
formal sense:

\begin{description}
\item[Motivation for stochastic array:]
  Suppose $\{y_t; t=1,...,n\}$ is a stochastic process where each
  $y_t$ is i.i.d standard normal. Let $s^2_n$ be the sample variance
  of this process. Is $z_t = (y_t - \bar y) / s$ a stochastic process?
  (no)

  So we introduce two sets of indices and write $z_{t,n} = (y_t - \bar
  y) / s_n$; where $n = 1,2,...$ and $t = 1,...,n$.  Now $\{z_{t,n}\}$
  is a stochastic array.
\item[Strict stationarity:]
  The stochastic process $\{y_t\}$ is a \textbf{strictly stationary
    time series} if, for all finite s, t, and k, we have
  \[(y_s,...,y_{s+k}) =^d (y_t,...,y_{t+k})\]

  Example of strict stationarity: suppose that $y_t$ is i.i.d.; then
  it is strictly stationary.

  Another example: suppose that, for any $t$, $(y_t, y_{t+1},
  y_{t+2})$ is distributed $N(μ,Σ)$, and that $y_t$ and $y_{t+3}$ are
  independent for all $t$; then $y_t$ is obviously strictly
  stationary.

  A counter-example. Suppose that $Δy_t$ is i.i.d. $N(0,1)$ and $y_0$
  is zero. Then $E y_i = 0$ for all $i$, but
  \[var(y_i) = \sum_{t=1}^i var(Δy_t) = i\]

  so the series is not stationary.

  Other examples too (e.g. breaks)
\end{description}

Strict stationary is usually ``unnecessarily'' restrictive; usually
asymptotic properties come from the first two moments, so we have a
variation of stationarity that applies to the first two moments.

\begin{description}
\item[Weak stationarity:] 
  The stochastic process $\{y_t\}$ is a \textbf{weakly stationary time
    series} if
  \[ E y_t = μ ∀t \] and 
  \[cov(y_t, y_{t+k}) = cov(y_s, y_{s+k}) ∀ \ s,t, k\] informally, the
  first two moments do not depend on $t$.
\item[Autocovariance, Autocorrelation:]
  The \textbf{autocovariance} function of a weakly stationary time
  series is defined as
  \[Γ(k) = cov(y_t, y_{t-k})\] (univariate or multivariate) The
  \textbf{autocorrelation} function of a weakly stationary time series
  is defined as
  \[Ρ(k) = var(y_t)^{-1} Γ(k)\]

  ``Weak dependence'' requires that $Γ(k) → 0$ as $k → ∞$.

  Obvious properties if $Γ(\cdot)$ is the autocovariance function of
  $y_t$:
  \begin{itemize}
  \item $var(y_t) = Γ(0)$
  \item $Γ(k) = Γ(-k)'$ for all $k$.
  \end{itemize}

  Properties if $y_t$ is univariate
  \begin{itemize} 
  \item $|γ(k)| ≤ γ(0)$ for all $k$.
  \item The autocovariance function is \emph{nonnegative definite}.
  \end{itemize}
\item[Nonnegative definite (extends concept for matrices):]

  A real-valued function f is nonnegative definite if, for any natural
  number $n$, any $a ∈ R^n$ and any $t ∈ Z^n$, we have
  \[\sum_{i=1}^n \sum_{j=1}^n a_i a_j f(t_i - t_j) ≥ 0.\]
\end{description}

In this part of the class, we are going to assume that the series are
already stationary (weak, usually) and are weakly-dependent.

\begin{itemize}
\item Statistical agencies typically provide seasonally-adjusted data
  and the data are usually aggregated enough that holidays, etc. are
  ignored.
\item Accounting for trends and permanent shocks is a big deal in
  macro; I'll try to add notes in the future.
\end{itemize}

\section{VAR and VMA processes}

\begin{itemize}
\item As a starting point, we are going to build dynamic processes as
  linear functions of ``white noise.''
\item Approach is analagous to what we did with OLS: we will talk
  about a statistical model that seems pretty narrow, but show how to
  get economic content out of it and why it might hold approximately
  in a general setting.
\end{itemize}

\begin{description}
\item[White noise; i.e. $WN(0,σ^2)$:]
  A stochastic process $\{e_t\}$ is $WN(0, σ^2)$ if each $e_t ∼ (0,
  σ^2)$ and $cov(e_t, e_s) = 0$ when $s ≠ t$.

  Obviously, the same definition applies when the $e_t$ are random
  vectors.
\end{description}

To build $y_t$ from a white noise process $e_t$, there are two obvious
approaches:
\begin{itemize}
\item Let $y_t$ depend on $e_t$ as well as past $e_s$ ($s < t$).
\item Let $y_t$ depend on past $y_s$ ($s < t$).
\item We're going to (for now) build $y_t$ from linear functions of
  $e_t$ and past $y_t$.
\end{itemize}

\begin{description}
\item[(Vector) Moving average process of order $q$:]

  $y_t$ is a VMA(q) if it is a stationary solution to the equation
  \[y_t = μ + e_t + \sum_{i=1}\^q θ_i e_{t-i}\] where $e_t$ is $WN(0,
  Σ)$, $μ$ is a constant $k$-vector, and $θ_i$ is a $k$ by $k$ matrix.

  Finding the moments of an $MA(q)$ process is easy and it is obvious
  that it is covariance stationary.

  \begin{itemize}
  \item $E y_t = μ + E e_t + \sum_{i=1}^q θ_i E e_{t-i} = μ$.
  \item $var(y_t) = var(e_t) + \sum_{i=1}^q θ_i var(e_{t-i}) θ_i' =
    \sum_{i=0}^q θ_i Σ θ_i'$ where $θ_0 = 1$.
  \item The autocovariances are straightforward too. Assume zero mean,
    then: 
    \[E y_t y_{t-j}' = E \sum_{i=0}^q θ_i e_{t-i} \sum_{k=0}^q e_{t-j-k}' θ_k' 
    = \sum_{i=j}^q θ_i E e_{t-i} e_{t-i}' θ_{i-j}'
    = \sum_{i=1}^q θ_i Σ θ_{i-j}'\] 
    This is zero if $i ≥ q$ (note that if $e_t$ is Normal, $MA(q)$
    processes are $q$-dependent).
  \end{itemize}
\item[(Vector) Autoregressive process of order p:]
  $y_t$ is a VAR(p) if it is a stationary solution to the difference
  equation 
  \[y_t = μ + \sum_{i=1}^p φ_i y_{t-1} + e_t\]
  where $e_t$ is $WN(0, Σ)$, $μ$ is a constant, and $φ_i$ are $k$ by
  $k$ matrices

  To get the mean, suppose that the process is covariance
  stationary. Then $E y_t = E y_{t-i}$ and so
  \[E y_t = μ + \sum_{i=1}^p φ_i E y_t + 0.\]

  If $I - \sum_{i=1}^p φ_i$ is invertible, then we have
  \[E y_t = (I - \sum_{i=1}^p φ_i )^{-1} μ\]
  (invertibility of this matrix turns out to be a necessary condition
  for covariance stationarity).

  For the autocovariances (which will give us the variance), assume
  0-mean, covariance stationary. Since
  \[y_t = \sum_{i=1}^p φ_i y_{t-i} + e_t,\]
  we can post-multiply by $y_{t-j}$ to get
  \[y_t y_{t-j}' = \sum_{i=1}^p φ_i y_{t-1} y_{t-j}' + e_t y_{t-j}'\]
  and (assuming $e_t$ and $y_{t-j}$ are uncorrelated)
  \[E y_t y_{t-j}' = \sum_{i=1}^p φ_i E y_{t-i} y_{t-j}'.\]

  This gives a recursive definition for the autocovariances. To get
  the initial conditions, we have $(j = 1,…,p)$
  \[Γ(0) = \sum_{i=1}^p φ_i Γ(i) + Σ Γ(j) = \sum_{i=1}^p φ_i Γ(j-i) \]

  Since $Γ(-j) = Γ(j)'$ this gives us a system of equations that can
  be solved for $Γ(0),…,Γ(p)$. Then, for higher-order autocovariances,
  we have the relationship (if $j ≥ p$)
  \[Γ(j) = \sum_{i=1}^p φ(i) Γ(j-i),\]
  which can be solved recursively or explicitly through difference
  equations.

  Note that the autocovariance dies out slowly over time for AR
  processes.
\item[Canonical representation of a VAR(p):]
If we define: 
\begin{align}
  w_t &= (y_t, y_{t-1}, …, y_{t-p+1})' \\
  u_t &= (e_t, 0, …, 0)' \\
  Ψ &= \begin{pmatrix}
    φ_1 & φ_2 & φ_3 & ⋯ & φ_{p-1} φ_p \\
    I & 0 & 0 & ⋯ & 0 & 0 \\
    0 & I & 0 & ⋯ & 0 & 0 \\
    ⋮ \\
    0 & 0 & 0 & ⋯ & I & 0
  \end{pmatrix}
\end{align}
Then we can write a VAR($p$) model as the VAR(1): 
\[w_t = Ψ w_{t-1} + e_t\]
\end{description}

\subsection{Lag polynomials:}

\begin{description}
\item[Lag operator:]

  The lag operator $L$ is defined as the operator s.t. $L y_t =
  y_{t-1}$

  The inverse is $L^{-1} y_t = y_{t+1}$.

  Behaves formally like premultiplication by a constant:
  \begin{itemize}
  \item $c ∙ (L y_t) = (c y_t)$
  \item $a + L y_t = L (a + y_t)$
  \item $L^k y_t = L(⋯ L(L(L y_t)) ⋯) = L(⋯ L(L y_{t-1}) ⋯) = y_{t-k}$
  \end{itemize}
  Lets us write ``lag polynomials'': 
  if $θ(z) = \sum_{i=0}^k θ_i z^i$ then 
  \[θ(L) y_t = \sum_{i=0}^k θ_i y_{t-i}.\]
\end{description}

Notice that we can write the AR(p) process as
\[φ(L) y_{t} = μ + e_t \]
where $φ(L) = I - \sum_{i=1}^p φ_i L^i$.

and the MA(q) as
\[y_t = μ + θ(L) e_t\]
where $θ(L) = I + \sum_{i=1}^q θ_i L^i$.

The notation suggests that we can ``solve'' for $y_t$ or ``solve'' for
$e_t$:
\[y_t = φ(L)^{-1} μ + φ(L)^{-1} θ(L) e_t\]
(making an MA process, possibly MA(∞)) or
\[θ(L)^{-1} φ(L) y_t = θ(L)^{-1} μ + e_t\]
(making an AR process, possibly AR(∞)).

Under some restrictions on the polynomials, we can do this; it's
essentially the same as taking a Taylor-series approximation for the
polynomial $θ(z)^{-1} or φ(z)^{-1}$.

\begin{itemize}
\item Suppose that $y_t$ satisfies $φ(L) y_t = e_t$. If $det φ(z) ≠ 0$
  for all (complex) $z$ s.t. $|z| ≤ 1$, then $y_t$ is covariance
  stationary and can be expressed as the MA(∞):
  \[y_t = φ(L)^{-1} e_t.\]
  \begin{itemize}
  \item This is called a \emph{causal} autoregressive process
  \item \emph{unit root}(s) if $det φ(z) = 0$ for $|z| = 1$; number of
    ``unit roots'' is the order of the solution.
  \item Give AR(1) example
  \item Mention non-causal AR processes as well
  \end{itemize}
\item Suppose that $y_t$ is the MA($q$) process $y_t = θ(L) e_t$. If
  $det θ(L) ≠ 0$ for all complex $z$ s.t. $|z| ≤ 1$, then $y_t$ can be
  expressed as the AR(∞) process: $θ(L)^{-1} y_t = e_t$

  \begin{itemize}
  \item This is called an \emph{invertible} moving average process
  \end{itemize}
\item Both of these results hold even if $φ$ or $θ$ are infinite-order
  polynomials.
\end{itemize}

\subsection{VARMA(p,q) models}

\begin{description}
\item[(V)ARMA(p, q):] $y_t$ is a $VARMA(p,q)$ process if it is the
  stationary solution to the difference equation $φ(L) y_t = μ + θ(L)
  e_t$ where $e_t$ is $WN(0, σ^2)$ and $μ$ is a constant, $φ$ and $θ$
  are polynomials of order $p$ and $q$ such that $φ(0) = θ(0) = I$.

  Note that if $φ(z)$ and $θ(z)$ have the same roots/zeros, so $φ(L) =
  a(L) b(L)$ and $θ(L) = a(L) c(L)$ then we can typically factor out
  and remove $a(L)$, giving a VARMA process with the same dynamics:
  $b(L) y_t = μ^* + c(L) e_t$; so we typically assume that there are
  no common roots

  Same conditions as above apply if we want causality and
  invertiblity.
\end{description}

\subsection{Dynamics:}

For \textbf{univariate} time series, people typically focus on
\begin{itemize}
\item Autocorrelation Function (ACF)
\item Partial Autocorrelation Function (PACF)
\end{itemize}
This holds whether or not the true DGP is thought to be an AR or MA
process.

\begin{description}
\item[ACF:]
  The Autocorrelation function is just $ρ(j)$ viewed as a function of
  $j$; we've already calculated the autocovariance for $AR(p)$ and
  $MA(q)$ processes, so the autocorrelation is just a matter of
  scaling.
\item[PACF:]
  The $j$th partial autocorrelation of a weakly stationary series is
  defined as $corr(y_t, y_{t-j} ∣ y_{t-1},…, y_{t-j+1})$.

  Define this as $α(j)$ for now

  The partial autocorrelation function is $α(j)$ as a function of $j$.

  Analogously to OLS, we know that $α(j)$ is the coefficient $β_j$ on
  $y_{t-j}$ in the equation
  \[y_t = β_0 + β_1 y_{t-1} + β_2 y_{t-2} + ⋯ + β_j y_{t-j} + u_t\]
  where the $β_i$ minimize the population MSE.

  Solution method: $(α(0), α(1)) = (γ(0), γ(1))$. Then it works just
  like for OLS:
  \[ y_t = ( y_{t-1}, y_{t-2},…, y_{t-i} ) β(i) + u_{t,i} \] giving
  \[( y_{t-1},…, y_{t-i} )'y_t = ( y_{t-1},…, y_{t-i} )' ( y_{t-1},…,
  y_{t-i} ) β(i) + ( y_{t-1},…, y_{t-i} )' u_{t,i}\]
  Take expectations and solve to get
  \begin{equation}
    \begin{pmatrix}
      γ(0) & γ(1) & ⋯ & γ(i) \\
      γ(1) & γ(0) & ⋯ & γ(i-1) \\
      ⋮ \\
      γ(i) & γ(i-1) & ⋯ & γ(0)
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      γ(1) \\ γ(2) \\ ⋮ \\ γ(i+1)
    \end{pmatrix}
    = 
    \begin{pmatrix}
      β_{i1} \\ β_{i2} \\ ⋮ \\ β_{ii}
    \end{pmatrix}
  \end{equation}
  and take the last element of $β(i)$ as $α(i)$.

  For an $AR(p)$, $α(i) = 0$ for all $ ≥ 0$

  For an $MA(q)$, the PACF dies out slowly as $i → ∞$.
\end{description}

\begin{itemize}
\item You can trivially extend ACF and PACF to vector processes and
  the same results broadly hold.
\item We're not going to worry about that now, because for
  multivariate dynamics we care mostly about finding structure that
  has an economic interpretation.
\item Sample ACF and PACF can be used for modeling.
\end{itemize}

\section{Wold Decomposition: the generality of VAR, VMA, and VARMA
  processes}

Introduce some probabilistic notation: define $F_t$ to be the sigma-field
(information set) generated by $y_t$, $y_{t-1}$, $y_{t-2},…$

\begin{description}
\item[Filtration:]
  A sequence $\{F_t\}$ of sigma-fields is a \emph{filtration} if
  $F_{t-1} ⊂ F_t$ for all $t$.

  The tail sigma-field of a collection of sigma-fields $\{F_t;
  t=…,-2,-1,0,1,2,…\}$ is defined as $⋂_{n=-∞}^∞ ⋃_{t=n}^∞ F_t$

  For a filtration, we have $⋃_{t=n}^{n+k} F_t = F_{n+k}$ and
  $⋃_{t=n}^k ⋂_{t=n+1}^k F_t = ⋃_{t=n+1}^k F_t$, so the tail
  sigma-field is equivalent to
  \begin{align}
    ⋂_{n=-∞}^∞ \lim_{k → ∞} ⋃_{t=n}^{n+k} F_t 
    &= ⋂_{n=-∞}^∞ \lim_{k → ∞} F_{n+k} \\
    &= \lim_{l,k → ∞} ⋂_{n=-l}^∞ F_{n+k} \\
    &= \lim_{l,k → ∞} F_{-l} \\
    &= \lim_{l → -∞} F_l \\
    &= F_{-∞}
  \end{align}

\item[Deterministic process:]
  A process is deterministic if it is perfectly predictable by linear
  combinations; i.e. $v_t$ is $G_{-∞}$-measurable, with $G_{-∞} = \lim
  G_n$ and
  \[G_n = \{c + \sum_{t ≤ n} a_t X_t ∣ c, a_t ∈ R\}.\]
\item[Wold decomposition:]
Any zero-mean nondeterministic stationary process $\{y_t\}$ can be
expressed as the sum of an $MA(∞)$ process $u_t$ and a deterministic
process $v_t$, so
\[y_t = \sum_{j=0}^∞ θ_j z_{t-j} + v_t\]
with $z_{t-j}$ in $G_n$ and $v_t$ in $G_{-∞}$; $z_t$ is the residual
from projecting $y_t$ onto $G_{t-1}$, and $v_t = y_t - θ(L) z_t$

Now, if $θ(L)$ is invertible then we can write this as a causal VAR,
and if $θ(L)$ can be factored, we can write the process as a causal
VARMA.
\end{description}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../textbook"
%%% End: 