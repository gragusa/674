% Copyright © 2013-2014 Gray Calhoun

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Introduction to time series and ARMA modeling}

This material is covered extensively in standard textbooks,
i.e. \citet{Ha00} or \citet{Gr12}, and is covered especially well by
specialty time series books (\citealp{BD91} and \citealp{Ha94}).

\subsection{Basic definitions}

\begin{description}
\item[Stochastic process:]
A stochastic process is a collection of random variables (or
random vectors) indexed by a parameter $t$ indicating time.
\end{description}

Want to discuss how this can be viewed as a sequence of random
variables or as a random function of $t$ (sample path).

Often we need to deal with \textbf{triangular stochastic arrays} in a
formal sense:

\begin{description}
\item[Motivation for stochastic array:]
  Suppose $\{y_t; t=1,...,n\}$ is a stochastic process where each
  $y_t$ is i.i.d standard normal. Let $s^2_n$ be the sample variance
  of this process. Is $z_t = (y_t - \bar y) / s$ a stochastic process?
  (no)

  So we introduce two sets of indices and write $z_{t,n} = (y_t - \bar
  y) / s_n$; where $n = 1,2,...$ and $t = 1,...,n$.  Now $\{z_{t,n}\}$
  is a stochastic array.
\item[Strict stationarity:]
  The stochastic process $\{y_t\}$ is a \textbf{strictly stationary
    time series} if, for all finite s, t, and k, we have
  \[(y_s,...,y_{s+k}) =^d (y_t,...,y_{t+k})\]

  Example of strict stationarity: suppose that $y_t$ is i.i.d.; then
  it is strictly stationary.

  Another example: suppose that, for any $t$, $(y_t, y_{t+1},
  y_{t+2})$ is distributed $N(\mu,\Sigma)$, and that $y_t$ and $y_{t+3}$ are
  independent for all $t$; then $y_t$ is obviously strictly
  stationary.

  A counter-example. Suppose that $\Delta y_t$ is i.i.d. $N(0,1)$ and $y_0$
  is zero. Then $\E y_i = 0$ for all $i$, but
  \[\var(y_i) = \sum_{t=1}^i \var(\Delta y_t) = i\]

  so the series is not stationary.

  Other examples too (e.g. breaks)
\end{description}

Strict stationary is usually ``unnecessarily'' restrictive; usually
asymptotic properties come from the first two moments, so we have a
variation of stationarity that applies to the first two moments.

\begin{description}
\item[Weak stationarity:]
  The stochastic process $\{y_t\}$ is a \textbf{weakly stationary time
    series} if
  \[ \E y_t = \mu \forall t \] and
  \[\cov(y_t, y_{t+k}) = \cov(y_s, y_{s+k}) \forall\ s,t, k\] informally, the
  first two moments do not depend on $t$.
\item[Autocovariance, Autocorrelation:]
  The \textbf{autocovariance} function of a weakly stationary time
  series is defined as
  \[\Gamma(k) = \cov(y_t, y_{t-k})\] (univariate or multivariate) The
  \textbf{autocorrelation} function of a weakly stationary time series
  is defined as
  \[
  \Rho(k) = \var(y_t)^{-1} \Gamma(k)
  \]

  ``Weak dependence'' requires that $\Gamma(k) \to 0$ as $k \to \infty$.

  Obvious properties if $\Gamma(\cdot)$ is the autocovariance function of
  $y_t$:
  \begin{itemize}
  \item $\var(y_t) = \Gamma(0)$
  \item $\Gamma(k) = \Gamma(-k)'$ for all $k$.
  \end{itemize}

  Properties if $y_t$ is univariate
  \begin{itemize}
  \item $|\gamma(k)| \leq \gamma(0)$ for all $k$.
  \item The autocovariance function is \emph{nonnegative definite}.
  \end{itemize}
\item[Nonnegative definite (extends concept for matrices):]

  A real-valued function f is nonnegative definite if, for any natural
  number $n$, any $a \in R^n$ and any $t \in Z^n$, we have
  \[\sum_{i=1}^n \sum_{j=1}^n a_i a_j f(t_i - t_j) \geq 0.\]
\end{description}

In this part of the class, we are going to assume that the series are
already stationary (weak, usually) and are weakly-dependent.

\begin{itemize}
\item Statistical agencies typically provide seasonally-adjusted data
  and the data are usually aggregated enough that holidays, etc. are
  ignored.
\item Accounting for trends and permanent shocks is a big deal in
  macro; I'll try to add notes in the future.
\end{itemize}

\subsection{VAR and VMA processes}

\begin{itemize}
\item As a starting point, we are going to build dynamic processes as
  linear functions of ``white noise.''
\item Approach is analogous to what we did with OLS: we will talk
  about a statistical model that seems pretty narrow, but show how to
  get economic content out of it and why it might hold approximately
  in a general setting.
\end{itemize}

\begin{description}
\item[White noise; i.e. $WN(0,\sigma^2)$:]
  A stochastic process $\{e_t\}$ is $WN(0, \sigma^2)$ if each $e_t ∼ (0,
  \sigma^2)$ and $\cov(e_t, e_s) = 0$ when $s \neq t$.

  Obviously, the same definition applies when the $e_t$ are random
  vectors.
\end{description}

To build $y_t$ from a white noise process $e_t$, there are two obvious
approaches:
\begin{itemize}
\item Let $y_t$ depend on $e_t$ as well as past $e_s$ ($s < t$).
\item Let $y_t$ depend on past $y_s$ ($s < t$).
\item We're going to (for now) build $y_t$ from linear functions of
  $e_t$ and past $y_t$.
\end{itemize}

\begin{description}
\item[(Vector) Moving average process of order $q$:]

  $y_t$ is a VMA(q) if it is a stationary solution to the equation
  \[y_t = \mu + e_t + \sum_{i=1}^q \theta_i e_{t-i}\] where $e_t$ is $WN(0,
  \Sigma)$, $\mu$ is a constant $k$-vector, and $\theta_i$ is a $k$ by $k$ matrix.

  Finding the moments of an $MA(q)$ process is easy and it is obvious
  that it is covariance stationary.

  \begin{itemize}
  \item $\E y_t = \mu + \E e_t + \sum_{i=1}^q \theta_i \E e_{t-i} = \mu$.
  \item $\var(y_t) = \var(e_t) + \sum_{i=1}^q \theta_i \var(e_{t-i}) \theta_i' =
    \sum_{i=0}^q \theta_i \Sigma \theta_i'$ where $\theta_0 = 1$.
  \item The autocovariances are straightforward too. Assume zero mean,
    then:
    \[\E y_t y_{t-j}' = \E \sum_{i=0}^q \theta_i e_{t-i} \sum_{k=0}^q e_{t-j-k}' \theta_k'
    = \sum_{i=j}^q \theta_i \E e_{t-i} e_{t-i}' \theta_{i-j}'
    = \sum_{i=1}^q \theta_i \Sigma \theta_{i-j}'\]
    This is zero if $i \geq q$ (note that if $e_t$ is Normal, $MA(q)$
    processes are $q$-dependent).
  \end{itemize}
\item[(Vector) Autoregressive process of order p:]
  $y_t$ is a VAR(p) if it is a stationary solution to the difference
  equation
  \[y_t = \mu + \sum_{i=1}^p \phi_i y_{t-1} + e_t\]
  where $e_t$ is $WN(0, \Sigma)$, $\mu$ is a constant, and $\phi_i$ are $k$ by
  $k$ matrices

  To get the mean, suppose that the process is covariance
  stationary. Then $\E y_t = \E y_{t-i}$ and so
  \[\E y_t = \mu + \sum_{i=1}^p \phi_i \E y_t + 0.\]

  If $I - \sum_{i=1}^p \phi_i$ is invertible, then we have
  \[\E y_t = (I - \sum_{i=1}^p \phi_i )^{-1} \mu\]
  (invertibility of this matrix turns out to be a necessary condition
  for covariance stationarity).

  For the autocovariances (which will give us the variance), assume
  0-mean, covariance stationary. Since
  \[y_t = \sum_{i=1}^p \phi_i y_{t-i} + e_t,\]
  we can post-multiply by $y_{t-j}$ to get
  \[y_t y_{t-j}' = \sum_{i=1}^p \phi_i y_{t-1} y_{t-j}' + e_t y_{t-j}'\]
  and (assuming $e_t$ and $y_{t-j}$ are uncorrelated)
  \[\E y_t y_{t-j}' = \sum_{i=1}^p \phi_i \E y_{t-i} y_{t-j}'.\]

  This gives a recursive definition for the autocovariances. To get
  the initial conditions, we have $(j = 1,\dots,p)$
  \[\Gamma(0) = \sum_{i=1}^p \phi_i \Gamma(i) + \Sigma \Gamma(j) = \sum_{i=1}^p \phi_i \Gamma(j-i) \]

  Since $\Gamma(-j) = \Gamma(j)'$ this gives us a system of equations that can
  be solved for $\Gamma(0),\dots,\Gamma(p)$. Then, for higher-order autocovariances,
  we have the relationship (if $j \geq p$)
  \[\Gamma(j) = \sum_{i=1}^p \phi(i) \Gamma(j-i),\]
  which can be solved recursively or explicitly through difference
  equations.

  Note that the autocovariance dies out slowly over time for AR
  processes.
\item[Canonical representation of a VAR(p):]
If we define:
\begin{align}
  w_t &= (y_t, y_{t-1}, \dots, y_{t-p+1})' \\
  u_t &= (e_t, 0, \dots, 0)' \\
  \Psi &= \begin{pmatrix}
    \phi_1 & \phi_2 & \phi_3 & \cdots & \phi_{p-1} \phi_p \\
    I & 0 & 0 & \cdots & 0 & 0 \\
    0 & I & 0 & \cdots & 0 & 0 \\
    \vdots \\
    0 & 0 & 0 & \cdots & I & 0
  \end{pmatrix}
\end{align}
Then we can write a VAR($p$) model as the VAR(1):
\[w_t = \Psi w_{t-1} + e_t\]
\end{description}

\subsection{Lag polynomials:}

\begin{description}
\item[Lag operator:]

  The lag operator $L$ is defined as the operator s.t. $L y_t =
  y_{t-1}$

  The inverse is $L^{-1} y_t = y_{t+1}$.

  Behaves formally like premultiplication by a constant:
  \begin{itemize}
  \item $c \cdot (L y_t) = (c y_t)$
  \item $a + L y_t = L (a + y_t)$
  \item $L^k y_t = L(\cdots L(L(L y_t)) \cdots) = L(\cdots L(L y_{t-1}) \cdots) = y_{t-k}$
  \end{itemize}
  Lets us write ``lag polynomials'':
  if $\theta(z) = \sum_{i=0}^k \theta_i z^i$ then
  \[\theta(L) y_t = \sum_{i=0}^k \theta_i y_{t-i}.\]
\end{description}

Notice that we can write the AR(p) process as
\[\phi(L) y_{t} = \mu + e_t \]
where $\phi(L) = I - \sum_{i=1}^p \phi_i L^i$.

and the MA(q) as
\[y_t = \mu + \theta(L) e_t\]
where $\theta(L) = I + \sum_{i=1}^q \theta_i L^i$.

The notation suggests that we can ``solve'' for $y_t$ or ``solve'' for
$e_t$:
\[y_t = \phi(L)^{-1} \mu + \phi(L)^{-1} \theta(L) e_t\]
(making an MA process, possibly MA($\infty$)) or
\[\theta(L)^{-1} \phi(L) y_t = \theta(L)^{-1} \mu + e_t\]
(making an AR process, possibly AR($\infty$)).

Under some restrictions on the polynomials, we can do this; it's
essentially the same as taking a Taylor-series approximation for the
polynomial $\theta(z)^{-1} or \phi(z)^{-1}$.

\begin{itemize}
\item Suppose that $y_t$ satisfies $\phi(L) y_t = e_t$. If $\det \phi(z) \neq 0$
  for all (complex) $z$ s.t. $|z| \leq 1$, then $y_t$ is covariance
  stationary and can be expressed as the MA($\infty$):
  \[y_t = \phi(L)^{-1} e_t.\]
  \begin{itemize}
  \item This is called a \emph{causal} autoregressive process
  \item \emph{unit root}(s) if $det \phi(z) = 0$ for $|z| = 1$; number of
    ``unit roots'' is the order of the solution.
  \item Give AR(1) example
  \item Mention non-causal AR processes as well
  \end{itemize}
\item Suppose that $y_t$ is the MA($q$) process $y_t = \theta(L) e_t$. If
  $\det \theta(L) \neq 0$ for all complex $z$ s.t. $|z| \leq 1$, then $y_t$ can be
  expressed as the AR($\infty$) process: $\theta(L)^{-1} y_t = e_t$

  \begin{itemize}
  \item This is called an \emph{invertible} moving average process
  \end{itemize}
\item Both of these results hold even if $\phi$ or $\theta$ are infinite-order
  polynomials.
\end{itemize}

\subsection{VARMA(p,q) models}

\begin{description}
\item[(V)ARMA(p, q):] $y_t$ is a $VARMA(p,q)$ process if it is the
  stationary solution to the difference equation $\phi(L) y_t = \mu + \theta(L)
  e_t$ where $e_t$ is $WN(0, \sigma^2)$ and $\mu$ is a constant, $\phi$ and $\theta$
  are polynomials of order $p$ and $q$ such that $\phi(0) = \theta(0) = I$.

  Note that if $\phi(z)$ and $\theta(z)$ have the same roots/zeros, so $\phi(L) =
  a(L) b(L)$ and $\theta(L) = a(L) c(L)$ then we can typically factor out
  and remove $a(L)$, giving a VARMA process with the same dynamics:
  $b(L) y_t = \mu^* + c(L) e_t$; so we typically assume that there are
  no common roots

  Same conditions as above apply if we want causality and
  invertiblity.
\end{description}

\subsection{Dynamics:}

For \textbf{univariate} time series, people typically focus on
\begin{itemize}
\item Autocorrelation Function (ACF)
\item Partial Autocorrelation Function (PACF)
\end{itemize}
This holds whether or not the true DGP is thought to be an AR or MA
process.

\begin{description}
\item[ACF:]
  The Autocorrelation function is just $\rho(j)$ viewed as a function of
  $j$; we've already calculated the autocovariance for $AR(p)$ and
  $MA(q)$ processes, so the autocorrelation is just a matter of
  scaling.
\item[PACF:]
  The $j$th partial autocorrelation of a weakly stationary series is
  defined as $\corr(y_t, y_{t-j} ∣ y_{t-1},\dots, y_{t-j+1})$.

  Define this as $\alpha(j)$ for now

  The partial autocorrelation function is $\alpha(j)$ as a function of $j$.

  Analogously to OLS, we know that $\alpha(j)$ is the coefficient $\beta_j$ on
  $y_{t-j}$ in the equation
  \[y_t = \beta_0 + \beta_1 y_{t-1} + \beta_2 y_{t-2} + \cdots + \beta_j y_{t-j} + u_t\]
  where the $\beta_i$ minimize the population MSE.

  Solution method: $(\alpha(0), \alpha(1)) = (\gamma(0), \gamma(1))$. Then it works just
  like for OLS:
  \[ y_t = ( y_{t-1}, y_{t-2},\dots, y_{t-i} ) \beta(i) + u_{t,i} \] giving
  \[( y_{t-1},\dots, y_{t-i} )'y_t = ( y_{t-1},\dots, y_{t-i} )' ( y_{t-1},\dots,
  y_{t-i} ) \beta(i) + ( y_{t-1},\dots, y_{t-i} )' u_{t,i}\]
  Take expectations and solve to get
  \begin{equation}
    \begin{pmatrix}
      \gamma(0) & \gamma(1) & \cdots & \gamma(i) \\
      \gamma(1) & \gamma(0) & \cdots & \gamma(i-1) \\
      \vdots \\
      \gamma(i) & \gamma(i-1) & \cdots & \gamma(0)
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      \gamma(1) \\ \gamma(2) \\ \vdots \\ \gamma(i+1)
    \end{pmatrix}
    =
    \begin{pmatrix}
      \beta_{i1} \\ \beta_{i2} \\ \vdots \\ \beta_{ii}
    \end{pmatrix}
  \end{equation}
  and take the last element of $\beta(i)$ as $\alpha(i)$.

  For an $AR(p)$, $\alpha(i) = 0$ for all $ \geq 0$

  For an $MA(q)$, the PACF dies out slowly as $i \to \infty$.
\end{description}

\begin{itemize}
\item You can trivially extend ACF and PACF to vector processes and
  the same results broadly hold.
\item We're not going to worry about that now, because for
  multivariate dynamics we care mostly about finding structure that
  has an economic interpretation.
\item Sample ACF and PACF can be used for modeling.
\end{itemize}

\subsection{Wold Decomposition: the generality of VAR, VMA, and VARMA
  processes}

Introduce some probabilistic notation: define $F_t$ to be the sigma-field
(information set) generated by $y_t$, $y_{t-1}$, $y_{t-2},\dots$

\begin{description}
\item[Filtration:]
  A sequence $\{F_t\}$ of sigma-fields is a \emph{filtration} if
  $F_{t-1} ⊂ F_t$ for all $t$.

  The tail sigma-field of a collection of sigma-fields $\{F_t;
  t=\dots,-2,-1,0,1,2,\dots\}$ is defined as $\bigcap_{n=-\infty}^\infty \bigcup_{t=n}^\infty F_t$

  For a filtration, we have $\bigcup_{t=n}^{n+k} F_t = F_{n+k}$ and
  $\bigcup_{t=n}^k \bigcap_{t=n+1}^k F_t = \bigcup_{t=n+1}^k F_t$, so the tail
  sigma-field is equivalent to
  \begin{align}
    \bigcap_{n=-\infty}^\infty \lim_{k \to \infty} \bigcup_{t=n}^{n+k} F_t
    &= \bigcap_{n=-\infty}^\infty \lim_{k \to \infty} F_{n+k} \\
    &= \lim_{l,k \to \infty} \bigcap_{n=-l}^\infty F_{n+k} \\
    &= \lim_{l,k \to \infty} F_{-l} \\
    &= \lim_{l \to -\infty} F_l \\
    &= F_{-\infty}
  \end{align}

\item[Deterministic process:]
  A process is deterministic if it is perfectly predictable by linear
  combinations; i.e. $v_t$ is $G_{-\infty}$-measurable, with $G_{-\infty} = \lim
  G_n$ and
  \[G_n = \{c + \sum_{t \leq n} a_t X_t ∣ c, a_t \in R\}.\]
\item[Wold decomposition:]
Any zero-mean nondeterministic stationary process $\{y_t\}$ can be
expressed as the sum of an $MA(\infty)$ process $u_t$ and a deterministic
process $v_t$, so
\[y_t = \sum_{j=0}^\infty \theta_j z_{t-j} + v_t\]
with $z_{t-j}$ in $G_n$ and $v_t$ in $G_{-\infty}$; $z_t$ is the residual
from projecting $y_t$ onto $G_{t-1}$, and $v_t = y_t - \theta(L) z_t$

Now, if $\theta(L)$ is invertible then we can write this as a causal VAR,
and if $\theta(L)$ can be factored, we can write the process as a causal
VARMA.
\end{description}

\subsection{Parameter estimation}

There are a few different approaches that we can take for estimation
\begin{itemize}
\item Method of moments
\item Maximum Likelihood (and conditional MLE)
\item Bayesian estimation
\end{itemize}

\subsection{Estimation for VAR parameters (no MA component).}

Have the model (again):
\[y_t = \mu + \sum_{i=1}^p \phi_i y_{t-i} + e_t\] $k$ equations, $k p + 1$
regression coefficients. Under assumptions of causality, we know that
$e_t$ is uncorrelated with $y_s$ for $s < t$, which implies that we
might be able to justify OLS:
\begin{itemize}
\item Let $z_t' = (1 y_{t-1}' \cdots y_{t-p}')$
\item $\Phi = [ \mu \phi_1 \dots \phi_p ]'$
\item Then $y_t' = z_t'\Phi' + e_t'$ and we can do equation-by-equation
  OLS, which is equivalent to
  \[\hat \Phi' = (\sum_{t=p+1}^T z_t z_t')^{-1} \sum_{t=p+1}^T z_t y_t'\]
\end{itemize}

This is the method of moments estimator as well as the conditional MLE
estimator if you assume that $e_t ∼ N(0, \Sigma)$ and
condition on $y_1$,\dots,$y_p$. To do MLE, observe that
\[y_t ∣ z_t ∼ N(\Phi z_t, \Sigma).\]

The joint likelihood can be taken as the product of the conditional
likelihoods:
\[L(\Phi, \Sigma; y_1,\dots,y_T) = f_T(y_T ∣ y_{T-1},\dots, y_1) f_{T-1}(y_{T-1} ∣
y_{T-2},\dots, y_1) \cdots f_{p+1}(y_{p+1} ∣ y_p,\dots,y_1) f_p(y_p,\dots,y_1)\]

Assuming normality and correct specification,
\[f_t(y_t ∣ y_{t-1},\dots, y_1) = f_t(y_t ∣ z_t).\]

If you want, you can avoid conditioning on $y_p$,\dots,$y_1$ since the
joint dist of those observations is also normal and we've worked out
how to find the mean and variance.

\subsection{Estimating MA models}

\begin{itemize}
\item This is a subject covered extensively by \citet{Ha94}, so we won't go
  into much detail here.
\item There are two basic approaches:
  \begin{enumerate}
  \item Assume $e_t$ is normal, then $y_1,\dots,y_T$ is jointly normal
    and you can maximize the likelihood w/rt $\Phi$, $\Theta$, and
    $\Sigma$

    This amounts to invoking invertibility to derive $f_T(y_t ∣
    y_{t-1},\dots,y_1)$ efficiently for each $t$ w/out using strict
    VAR structure, then building up the unconditional likelihood from
    that.

  \item Just like for VAR and VMA processes, we can derive equations
    that define the autocovariances at lags $0,\dots,p+q$ in terms of
    $\Phi$ and $\Theta$; solving for $\Phi$ and $\Theta$ and plugging
    in the sample autocovariances give the ``Yule-Walker'' estimates.

    This would be OLS for a VAR; when there's an MA component it can
    be inefficient.
  \end{enumerate}
\end{itemize}

\subsection{Asymptotic theory for time-series}

Look at OLS estimator for VAR(p). As always, two components:
\[\hat \Phi' = \Phi' + (1/T \sum_{t=p+1}^T z_t z_t')^{-1} 1/T
\sum_{t=p+1}^T z_t e_t'\] so, for consistency need the sum of $z$'s to be
$O_p(1)$; second sum to be $o_p(1)$. For asymptotic distribution
(normality) need second summation to obey a CLT.

\begin{itemize}
\item Obviously, if we want to estimate the first or second moments of
  a time-series process, we might want to use the sample moments.
\item Of interest on their own
\item Necessary for consistency and asymptotic normality of MoM
  estimators
\item Just look at univariate for simplicity.
\end{itemize}

\begin{description}
\item[Filtration:]
  A sequence of sigma-fields $\{G_t\}$ is a \emph{filtration} if $G_t
  ⊂ G_{t+1}$ for all $t$
\item[Martingale difference sequence:]
  The sequence of rvs $\{e_t\}$ is an mds w/rt the filtration $\{G_t\}$ if
  $E(e_t ∣ G_{t-1}) = 0$.

  Typically $G_t = \sigma(e_t, e_{t-1},\dots)$

  Comes from the definition of a martingale: $y_t$ is a martingale
  w/rt the filtration if $E(y_t ∣ G_{t-1}) = y_{t-1}$. Then $e_t =
  \Delta y_t$ is an mds.
\end{description}

\begin{itemize}
\item Now, suppose that $e_t$ has finite variance and is an mds w/rt
  $G_t = \sigma(y_t, y_{t-1},\dots)$, then we know that the second sum has mean
  zero. Also
  \begin{align*}
    E (1/T \sum z_t e_t)(1/T \sum e_s z_s')
    &= 2/T^2 \sum_{s \leq t} E(e_s e_t z_t z_s' ∣ G_{t-1}) \\
    &= 1/T^2 \sum_t E E( e_t^2 ∣ G_{t-1}) z_t z_t' + 2/T^2 \sum_{s < t} E E(
    e_t ∣ G_{t-1}) e_s z_t z_s' \\
    &= \sigma^2 1/T^2 \sum_t E z_t z_t'
  \end{align*}

  This converges to zero since $y_t$ has finite variance and $\Phih$
  converges in MSE to $\Phi$.
\item Note that if $e_t$ is MDS, so is $z_t e_t$
\end{itemize}

\subsection{Standard MDS CLT (McLeish 1974; Hall and Heyde 1980)}

Suppose that $Z_t$ is a (univariate) MDS and define
\begin{itemize}
\item $U_n^2 = \sum_1^n Z_t^2$,
\item $s_n^2 = E U_n^2$.
\end{itemize}
If
\begin{itemize}
\item $U_n^2 / s_n^2 \to 1$ i.p. and
\item $max_{1 \leq t \leq n} |Z_n / s_n| \to 0$ i.p.
\end{itemize}
then
\[1/s_n \sum_1^n Z_t \to^d N(0, 1)\]
or, equivalently,
\[1/U_n \sum Z_t \to^d N(0, 1)\]

For multivariate, let $\Omega_n = \sum E Z_t Z_t'$ and we have $\Omega_n^{-1/2} \sum
Z_t \to N(0, I)$ under essentially the same conditions.

Apply this to second summation in OLS coefficients and (assuming the
conditions hold) we can see that
\[n^{-1/2} \sum z_t e_t = ( \Omega_n / n )^{1/2} \Omega_n^{-1/2} \sum z_t e_t
\to^d N(0,\Omega) \]

where $\Omega_n = \sum E( e_t^2 z_t z_t' )$ and $\Omega = \lim \Omega_n / n$. Under
homoskedasticity, this can be simplified.

If $1/n \sum z_t z_t' \to V$ i.p., then $\lim \Omega_n = \sigma^2 V$ under
homoskedasticity and we have
\[\sqrt{n} ( \hat \phi - \phi ) \to^d N( 0, \sigma^2 V^{-1} ).\]

Under heteroskedasticity,
\[\sqrt{n} (\hat \phi - \phi ) \to^d N( 0, V^{-1} \Omega V^{-1} )\]

\textbf{For the asymptotic distribution of the VAR(p)} obviously it's
joint normal, so the only tricky thing is accounting for all of the
correlations. Rewrite it as
\[
sqrt(n) (\hat \Phi' - \Phi')
= (1/n \sum_{t=p+1}^n z_t z_t')^{-1} 1/sqrt(n) \sum_{t=p+1}^n z_t e_t'
\]
so
\[\sqrt{n} ( vec(\hat \Phi' ) - vec( \Phi' ) ) = vec( \dots )\]

and use the fact that, for conformable matrices $A$, $B$, and $C$,
\[vec( A B C ) = ( C \otimes A ) vec( B )\]
where $\otimes$ is the kronecker product and
\[
C \otimes A =
\begin{pmatrix}
  c_{11}A & \cdots & c_{1n}A \\
  \vdots
  c_{n1}A & \cdots & c_{nn}A
\end{pmatrix}
\]
$A = ( 1/n \sum z_t z_t' )^{-1}$, $B = 1/sqrt{n} \sum z_t e_t'$, $C = I$ so
$C \otimes A = diag( A,\dots, A )$

if $V = plim 1/n \sum z_t z_t' \Omega = plim 1/n \sum vec( z_t e_t' ) vec( z_t
e_t' )'$, then $\sqrt{n} vec(\hat \Phi' - \Phi' ) \to N( 0, diag(
V^{-1},\dots, V^{-1} ) \Omega diag(V^{-1},\dots,V^{-1} )$

Note that the block diagonal structure means that if we're interested
in the joint dist. of the coefficients for just a single equation,
it's not affected by those from other equations (only holds when all
of the equations have the same regressors\dots).

\subsection{Result (in summary)}

Suppose that $y_t$ is a VAR(p),
\begin{itemize}
\item $1/n \sum z_t z_t' \to V$ i.p.
\item $1/n \sum z_t z_t' e_t^2 \to \Omega$ i.p.
\item $e_t$ is an mds with finite $4 + \delta$ moments ($\delta > 0$)
\end{itemize}
Then $\sqrt{T} ( \hat \Phi' - \Phi' )$ is asymptotically normal (with the
variance we derived).

\subsection{Laws of large numbers}
We still need to talk about the square terms:
\[1/n \sum z_t z_t'\]
and
\[1/n \sum vec( z_t e_t' ) vec( z_t e_t' )'\]
\begin{description}
\item[LLN for martingale difference sequences:]
If $z_t$ is uniformly integrable (i.e. a slightly weaker condition than
assuming it has $1 + \delta$ moments, $\delta > 0$) and MDS, then
\[1/n \sum z_t \to^{L_1} 0\]
\end{description}

MDS for second moments is not usually an implication of Economic
Theory; is not usually a feature of a correctly specified model of the
mean; and is often false in Economic data. So it's worth mentioning
another result:

\begin{description}
\item[LLN for stationary and ergodic sequences:]

  If $\{z_t\}$ is a stationary and ergodic sequence with finite mean
  $\mu$, then
  \[1/n \sum z_t \to \mu\] (in $L_1$).

  Ergodic: a stationary sequence $\{x_t\}$ is \emph{ergodic} if, for
  any bounded functions $f$ and $g$, as $n \to \infty$,
  \[E f( x_t, \dots, x_{t + k}) g(x_{t + n}, \dots, x_{ t + n + l} ) \to E f(
  x_t, \dots, x_{ t + k } ) E g(x_{ t + n }, \dots, x_{ t + n + l} )\]

  \begin{itemize}
  \item remember, you can prove independence by proving this
    factorization for all bounded $f$ and $g$, so this is an ``asymptotic
    independence'' condition;
  \item Sometimes see conditions on how fast the convergence to zero
    happens
  \item Nothing special about stationarity; you'll see other weak
    dependence conditions based on similar ideas (i.e. mixing
    conditions).
  \end{itemize}
\end{description}

\subsection{CLT with serial correlation}

Have a CLT for MDS. But what if we have an MA process?
\[y_t = \sum_{j=0}^\infty \theta_j e_{t-j} = \theta(L) e_t\]

\begin{itemize}
\item Just focus on the average: $\bar y$
\item Obviously, if $\theta_j$ decays fast enough, LLN holds.
\item Look at CLT:
  \begin{align*}
    T^{1/2} \bar y
    &= 1/T^{1/2} \sum_{t=1}^T \sum_{j=0}^\infty \theta_j e_{t-j} \\
    &= \sqrt{1/T} ( \theta_0 e_T +
    ( \theta_0 + \theta_1 ) e_{T-1} +
    ( \theta_0 + \theta_1 + \theta_2 )  e_{T-2} + ...
    ( \theta_0 + ... + \theta_{T-1} ) e_1 + ... ) \\
    &= \sqrt{1/T} ( ( \theta(1) - \sum_1^\infty \theta_j ) e_T +
    ( \theta(1) - \sum_2^\infty \theta_j ) e_{T-1} + ...
    ( \theta(1) - \sum_T^\infty \theta_j ) e_1 + ... ) \\
    &= \sqrt{1/T} \theta(1) \sum_{t=1}^T e_t + \sqrt{1/T} \theta^*(L) e_t
  \end{align*}
  where $\theta_j^* = - \sum_{i=j+1}^\infty \theta_i$.  Now $\theta(1)$ has the bulk of the
  dependence, so the first term obeys a CLT and the second can
  converge to zero i.p.
\end{itemize}

Result: Suppose that $y_t = \mu + \theta(L) e_t$ where
\begin{itemize}
\item $\sum_{s=0}^\infty s |\theta_s| < \infty$
\item $1/\sqrt{T} \sum_t e_t \to N(0, \sigma^2)$ in distribution (i.e. any CLT
  holds)
\end{itemize}
Then $\sqrt{T} (\bar y - \mu ) \to N( 0, \sigma^2 \theta(1)^2 )$ in distribution

\subsection{Last bits}

Forecasting from VAR:
\begin{itemize}
\item 1-step ahead $\hat y_{T+1} = \hat \phi(L) y_T$
\item $k$-step ahead $\hat y_{T+k} = \hat \phi(L) y_{T+k-1} = \dots = ( \hat
  \phi(L) )^k y_T$
\end{itemize}
Also see forecasting by just regressing $y_{t+k}$ on $y_t,\dots,y_{t-p}$

What to do with these approximations?
\begin{itemize}
\item Test as normal
\item Test for Granger causality/exogeneity
  \begin{itemize}
  \item split up our variables into
    \begin{align*}
      y_{1t} &= \mu_1 + A_1 x_{1t} + A_2 x_{2t} + e_t \\
      y_{2t} &= \mu_2 + B_1 x_{1t} + B_2 x_{2t} + e_t
    \end{align*}
    where the $x_{it}$ contains lags of $y_{it}$.
  \item $y_2$ does not Granger-cause $y_1$ if lags of $y_2$ do not
    help predict $y_1$ after accounting for lags of $y_1$ (also means
    that $y_1$ is exogenous w/rt $y_2$).
  \item Means that $A_2 = 0$.
  \item This is something we can test.
  \end{itemize}
\end{itemize}

\subsubsection{Lag length selection}

\begin{itemize}
\item $AIC_p = \log | \Sigma_u(p)| + 2 p m^2 / T$
\item $BIC_p = \log | \Sigma_u(p)| + \log(T) 2 p m^2 / T$
\item $\Sigma_u(q)$ is the vcv of $y_t - b_1 y_{t-1} - \dots - y_{t-p}$
\item Choose to minimize the criterion
\item Last term works as a penalty
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../macroeconometrics"
%%% End:
